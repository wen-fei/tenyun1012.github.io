<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>graph-based-deep-learning-literature</title>
      <link href="/2019/12/19/graph-based-deep-learning-literature/"/>
      <url>/2019/12/19/graph-based-deep-learning-literature/</url>
      
        <content type="html"><![CDATA[<h1 id="Graph-based-deep-learning-literature"><a href="#Graph-based-deep-learning-literature" class="headerlink" title="Graph-based deep learning literature"></a>Graph-based deep learning literature</h1><p>The repository contains links to</p><ul><li><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md" target="_blank" rel="noopener">conference publications</a> and <a href="https://github.com/naganandy/graph-based-deep-learning-literature#top-10-most-cited-publications" target="_blank" rel="noopener">the top 10 most cited publications</a></li><li><a href="https://github.com/naganandy/graph-based-deep-learning-literature#related-workshops" target="_blank" rel="noopener">related workshops</a></li><li><a href="https://github.com/naganandy/graph-based-deep-learning-literature#surveys--literature-reviews" target="_blank" rel="noopener">surveys / literature reviews</a></li></ul><p>in graph-based deep learning. The <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#conferences" target="_blank" rel="noopener">links to conference publications</a> are arranged in the reverse chronological order of conference dates from the conferences below. Please click on a year below beside a conference name to see publications of the conference in that year.</p><ul><li><h2 id="Machine-learning-conferences"><a href="#Machine-learning-conferences" class="headerlink" title="Machine learning conferences"></a>Machine learning conferences</h2><ul><li><h3 id="NeurIPS-2019-2018-2017-2016-2015"><a href="#NeurIPS-2019-2018-2017-2016-2015" class="headerlink" title="NeurIPS - 2019 | 2018 | 2017 | 2016 | 2015"></a><a href="https://nips.cc/" target="_blank" rel="noopener">NeurIPS</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_neurips19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_neurips18/README.md" target="_blank" rel="noopener">2018</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#nips-2017" target="_blank" rel="noopener">2017</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#nips-2016" target="_blank" rel="noopener">2016</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#nips-2015" target="_blank" rel="noopener">2015</a></h3></li><li><h3 id="ICML-2019-2018-2017-2016"><a href="#ICML-2019-2018-2017-2016" class="headerlink" title="ICML - 2019 | 2018 | 2017 | 2016"></a><a href="https://icml.cc/" target="_blank" rel="noopener">ICML</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_icml19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icml-2018-jul" target="_blank" rel="noopener">2018</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icml-2017" target="_blank" rel="noopener">2017</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icml-2016" target="_blank" rel="noopener">2016</a></h3></li><li><h3 id="ICLR-2019-2018-2017-2016-2014"><a href="#ICLR-2019-2018-2017-2016-2014" class="headerlink" title="ICLR - 2019 | 2018 | 2017 | 2016 | 2014"></a><a href="https://iclr.cc/" target="_blank" rel="noopener">ICLR</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_iclr19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2018-may" target="_blank" rel="noopener">2018</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2017" target="_blank" rel="noopener">2017</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2016" target="_blank" rel="noopener">2016</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2014" target="_blank" rel="noopener">2014</a></h3></li></ul></li></ul><ul><li><h2 id="Computer-vision-conferences"><a href="#Computer-vision-conferences" class="headerlink" title="Computer vision conferences"></a>Computer vision conferences</h2><ul><li><h3 id="CVPR-2019-2018-2017"><a href="#CVPR-2019-2018-2017" class="headerlink" title="CVPR - 2019 | 2018 | 2017"></a><a href="http://cvpr2020.thecvf.com/" target="_blank" rel="noopener">CVPR</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_cvpr19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#cvpr-2018-jun" target="_blank" rel="noopener">2018</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#cvpr-2017" target="_blank" rel="noopener">2017</a></h3></li><li><h3 id="ICCV-2019-2017"><a href="#ICCV-2019-2017" class="headerlink" title="ICCV - 2019 | 2017"></a><a href="http://iccv2019.thecvf.com/" target="_blank" rel="noopener">ICCV</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_iccv19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iccv-2017" target="_blank" rel="noopener">2017</a></h3></li><li><h3 id="ECCV-2018"><a href="#ECCV-2018" class="headerlink" title="ECCV - 2018"></a><a href="https://eccv2020.eu/" target="_blank" rel="noopener">ECCV</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#eccv-2018-sep" target="_blank" rel="noopener">2018</a></h3></li></ul></li></ul><ul><li><h2 id="Data-conferences"><a href="#Data-conferences" class="headerlink" title="Data conferences"></a>Data conferences</h2><ul><li><h3 id="KDD-2019-2018"><a href="#KDD-2019-2018" class="headerlink" title="KDD - 2019 | 2018"></a><a href="https://www.kdd.org/kdd2020/" target="_blank" rel="noopener">KDD</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_kdd19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#kdd-2018-aug" target="_blank" rel="noopener">2018</a></h3></li><li><h3 id="WWW-2019-2018"><a href="#WWW-2019-2018" class="headerlink" title="WWW - 2019 | 2018"></a><a href="https://www2020.thewebconf.org/" target="_blank" rel="noopener">WWW</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#www-2019-may" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#www-2018-april" target="_blank" rel="noopener">2018</a></h3></li><li><h3 id="ICDM-2019-2018"><a href="#ICDM-2019-2018" class="headerlink" title="ICDM - 2019 | 2018"></a><a href="http://icdm2019.bigke.org/" target="_blank" rel="noopener">ICDM</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icdm-2019-nov" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icdm-2018-nov" target="_blank" rel="noopener">2018</a></h3></li></ul></li></ul><ul><li><h2 id="Artificial-intelligence-conferences"><a href="#Artificial-intelligence-conferences" class="headerlink" title="Artificial intelligence conferences"></a>Artificial intelligence conferences</h2><ul><li><h3 id="AAAI-2020-2019-2018-2017"><a href="#AAAI-2020-2019-2018-2017" class="headerlink" title="AAAI - 2020 | 2019 | 2018 | 2017"></a><a href="https://aaai.org/Conferences/AAAI-20/" target="_blank" rel="noopener">AAAI</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_aaai20/README.md" target="_blank" rel="noopener">2020</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_aaai19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#aaai-2018-feb" target="_blank" rel="noopener">2018</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#aaai-2017" target="_blank" rel="noopener">2017</a></h3></li><li><h3 id="IJCAI-2019-2018-2017"><a href="#IJCAI-2019-2018-2017" class="headerlink" title="IJCAI - 2019 | 2018 | 2017"></a><a href="https://ijcai20.org/" target="_blank" rel="noopener">IJCAI</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_ijcai19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#ijcai-2018-jul" target="_blank" rel="noopener">2018</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#ijcai-2017" target="_blank" rel="noopener">2017</a></h3></li><li><h3 id="UAI-2019-2018"><a href="#UAI-2019-2018" class="headerlink" title="UAI - 2019 | 2018"></a><a href="http://auai.org/~w-auai/uai2020/" target="_blank" rel="noopener">UAI</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#uai-2019-jul" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#uai-2018-aug" target="_blank" rel="noopener">2018</a></h3></li></ul></li><li><h2 id="Natural-language-processing-conferences"><a href="#Natural-language-processing-conferences" class="headerlink" title="Natural language processing conferences"></a>Natural language processing conferences</h2><ul><li><h3 id="ACL-2019-2018"><a href="#ACL-2019-2018" class="headerlink" title="ACL - 2019 | 2018"></a><a href="https://acl2020.org/" target="_blank" rel="noopener">ACL</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_acl19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#acl-2018-jul" target="_blank" rel="noopener">2018</a></h3></li><li><h3 id="EMNLP-2019-2018-2017"><a href="#EMNLP-2019-2018-2017" class="headerlink" title="EMNLP - 2019 | 2018 | 2017"></a><a href="https://www.emnlp-ijcnlp2019.org/" target="_blank" rel="noopener">EMNLP</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_emnlp19/README.md" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#emnlp-2018-nov" target="_blank" rel="noopener">2018</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#emnlp-2017" target="_blank" rel="noopener">2017</a></h3></li><li><h3 id="NAACL-2019-2018"><a href="#NAACL-2019-2018" class="headerlink" title="NAACL - 2019 | 2018"></a><a href="https://naacl2019.org/" target="_blank" rel="noopener">NAACL</a> - <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#naacl-2019-jun" target="_blank" rel="noopener">2019</a> | <a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#naacl-2018-jun" target="_blank" rel="noopener">2018</a></h3></li></ul></li></ul><h1 id="Top-10-most-cited-publications"><a href="#Top-10-most-cited-publications" class="headerlink" title="Top 10 most cited publications"></a>Top 10 most cited publications</h1><ol><li><h3 id="Semi-Supervised-Classification-with-Graph-Convolutional-Networks"><a href="#Semi-Supervised-Classification-with-Graph-Convolutional-Networks" class="headerlink" title="Semi-Supervised Classification with Graph Convolutional Networks"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_pre18/gcn_iclr17/README.md" target="_blank" rel="noopener">Semi-Supervised Classification with Graph Convolutional Networks</a></h3></li><li><h3 id="Convolutional-Neural-Networks-on-Graphs-with-Fast-Localized-Spectral-Filtering"><a href="#Convolutional-Neural-Networks-on-Graphs-with-Fast-Localized-Spectral-Filtering" class="headerlink" title="Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_pre18/chebnet_nips16/README.md" target="_blank" rel="noopener">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a></h3></li><li><h3 id="Spectral-Networks-and-Locally-Connected-Networks-on-Graphs"><a href="#Spectral-Networks-and-Locally-Connected-Networks-on-Graphs" class="headerlink" title="Spectral Networks and Locally Connected Networks on Graphs"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_pre18/graphcnn_iclr14/README.md" target="_blank" rel="noopener">Spectral Networks and Locally Connected Networks on Graphs</a></h3></li><li><h3 id="Inductive-Representation-Learning-on-Large-Graphs"><a href="#Inductive-Representation-Learning-on-Large-Graphs" class="headerlink" title="Inductive Representation Learning on Large Graphs"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_pre18/graphsage_nips17/README.md" target="_blank" rel="noopener">Inductive Representation Learning on Large Graphs</a></h3></li><li><h3 id="Convolutional-Networks-on-Graphs-for-Learning-Molecular-Fingerprints"><a href="#Convolutional-Networks-on-Graphs-for-Learning-Molecular-Fingerprints" class="headerlink" title="Convolutional Networks on Graphs for Learning Molecular Fingerprints"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_pre18/graphcnn_nips15/README.md" target="_blank" rel="noopener">Convolutional Networks on Graphs for Learning Molecular Fingerprints</a></h3></li><li><h3 id="The-Graph-Neural-Network-Model"><a href="#The-Graph-Neural-Network-Model" class="headerlink" title="The Graph Neural Network Model"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnn_tnn09/README.md" target="_blank" rel="noopener">The Graph Neural Network Model</a></h3></li><li><h3 id="Graph-Attention-Networks"><a href="#Graph-Attention-Networks" class="headerlink" title="Graph Attention Networks"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_conf18/gan_iclr18/README.md" target="_blank" rel="noopener">Graph Attention Networks</a></h3></li><li><h3 id="Neural-Message-Passing-for-Quantum-Chemistry"><a href="#Neural-Message-Passing-for-Quantum-Chemistry" class="headerlink" title="Neural Message Passing for Quantum Chemistry"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_pre18/mpnn_icml17/README.md" target="_blank" rel="noopener">Neural Message Passing for Quantum Chemistry</a></h3></li><li><h3 id="Gated-Graph-Sequence-Neural-Networks"><a href="#Gated-Graph-Sequence-Neural-Networks" class="headerlink" title="Gated Graph Sequence Neural Networks"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_pre18/ggnn_iclr16/README.md" target="_blank" rel="noopener">Gated Graph Sequence Neural Networks</a></h3></li><li><h3 id="Learning-Convolutional-Neural-Networks-for-Graphs"><a href="#Learning-Convolutional-Neural-Networks-for-Graphs" class="headerlink" title="Learning Convolutional Neural Networks for Graphs"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_pre18/gcn_icml16/README.md" target="_blank" rel="noopener">Learning Convolutional Neural Networks for Graphs</a></h3></li></ol><h1 id="Related-workshops"><a href="#Related-workshops" class="headerlink" title="Related workshops"></a>Related workshops</h1><ul><li><h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><ul><li><h3 id="Deep-Learning-on-Graphs-Methodologies-and-Applications-AAAI"><a href="#Deep-Learning-on-Graphs-Methodologies-and-Applications-AAAI" class="headerlink" title="Deep Learning on Graphs: Methodologies and Applications (AAAI)"></a><a href="https://dlg2019.bitbucket.io/aaai20/" target="_blank" rel="noopener">Deep Learning on Graphs: Methodologies and Applications (AAAI)</a></h3></li></ul></li><li><h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><ul><li><h3 id="Graph-Representation-Learning-NeurIPS"><a href="#Graph-Representation-Learning-NeurIPS" class="headerlink" title="Graph Representation Learning (NeurIPS)"></a><a href="https://grlearning.github.io/" target="_blank" rel="noopener">Graph Representation Learning (NeurIPS)</a></h3></li><li><h3 id="Scene-Graph-Representation-and-Learning-ICCV"><a href="#Scene-Graph-Representation-and-Learning-ICCV" class="headerlink" title="Scene Graph Representation and Learning (ICCV)"></a><a href="https://cs.stanford.edu/people/ranjaykrishna/sgrl/index.html" target="_blank" rel="noopener">Scene Graph Representation and Learning (ICCV)</a></h3></li><li><h3 id="Deep-Learning-on-Graphs-Methods-and-Applications-KDD"><a href="#Deep-Learning-on-Graphs-Methods-and-Applications-KDD" class="headerlink" title="Deep Learning on Graphs: Methods and Applications (KDD)"></a><a href="https://dlg2019.bitbucket.io/" target="_blank" rel="noopener">Deep Learning on Graphs: Methods and Applications (KDD)</a></h3></li><li><h3 id="Learning-and-Reasoning-with-Graph-Structured-Representations-ICML"><a href="#Learning-and-Reasoning-with-Graph-Structured-Representations-ICML" class="headerlink" title="Learning and Reasoning with Graph-Structured Representations (ICML)"></a><a href="https://graphreason.github.io/" target="_blank" rel="noopener">Learning and Reasoning with Graph-Structured Representations (ICML)</a></h3></li><li><h3 id="Representation-Learning-on-Graphs-and-Manifolds-ICLR"><a href="#Representation-Learning-on-Graphs-and-Manifolds-ICLR" class="headerlink" title="Representation Learning on Graphs and Manifolds (ICLR)"></a><a href="https://rlgm.github.io/" target="_blank" rel="noopener">Representation Learning on Graphs and Manifolds (ICLR)</a></h3></li></ul></li><li><h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><ul><li><h3 id="Relational-Representation-Learning-NeurIPS"><a href="#Relational-Representation-Learning-NeurIPS" class="headerlink" title="Relational Representation Learning (NeurIPS)"></a><a href="https://r2learning.github.io/" target="_blank" rel="noopener">Relational Representation Learning (NeurIPS)</a></h3></li></ul></li></ul><h1 id="Surveys-literature-reviews"><a href="#Surveys-literature-reviews" class="headerlink" title="Surveys / literature reviews"></a>Surveys / literature reviews</h1><ul><li><h2 id="2019-1"><a href="#2019-1" class="headerlink" title="2019"></a>2019</h2><ul><li><h3 id="Graph-Neural-Networks-for-Small-Graph-and-Giant-Network-Representation-Learning-An-Overview"><a href="#Graph-Neural-Networks-for-Small-Graph-and-Giant-Network-Representation-Learning-An-Overview" class="headerlink" title="Graph Neural Networks for Small Graph and Giant Network Representation Learning: An Overview"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnnaug_arxiv19/README.md" target="_blank" rel="noopener">Graph Neural Networks for Small Graph and Giant Network Representation Learning: An Overview</a></h3></li><li><h3 id="Learning-Representations-of-Graph-Data-–-A-Survey"><a href="#Learning-Representations-of-Graph-Data-–-A-Survey" class="headerlink" title="Learning Representations of Graph Data – A Survey"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/lrg_arxiv19/README.md" target="_blank" rel="noopener">Learning Representations of Graph Data – A Survey</a></h3></li></ul></li><li><h2 id="2018-1"><a href="#2018-1" class="headerlink" title="2018"></a>2018</h2><ul><li><h3 id="Adversarial-Attack-and-Defense-on-Graph-Data-A-Survey"><a href="#Adversarial-Attack-and-Defense-on-Graph-Data-A-Survey" class="headerlink" title="Adversarial Attack and Defense on Graph Data: A Survey"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/aagsurvey_arxiv18/README.md" target="_blank" rel="noopener">Adversarial Attack and Defense on Graph Data: A Survey</a></h3></li><li><h3 id="Graph-Neural-Networks-A-Review-of-Methods-and-Applications"><a href="#Graph-Neural-Networks-A-Review-of-Methods-and-Applications" class="headerlink" title="Graph Neural Networks: A Review of Methods and Applications"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnnreview_arxiv18/README.md" target="_blank" rel="noopener">Graph Neural Networks: A Review of Methods and Applications</a></h3></li><li><h3 id="A-Comprehensive-Survey-on-Graph-Neural-Networks"><a href="#A-Comprehensive-Survey-on-Graph-Neural-Networks" class="headerlink" title="A Comprehensive Survey on Graph Neural Networks"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnnsurvey_arxiv19/README.md" target="_blank" rel="noopener">A Comprehensive Survey on Graph Neural Networks</a></h3></li><li><h3 id="Deep-Learning-on-Graphs-A-Survey"><a href="#Deep-Learning-on-Graphs-A-Survey" class="headerlink" title="Deep Learning on Graphs: A Survey"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/dlgsurvey_arxiv18/README.md" target="_blank" rel="noopener">Deep Learning on Graphs: A Survey</a></h3></li><li><h3 id="Relational-inductive-biases-deep-learning-and-graph-networks"><a href="#Relational-inductive-biases-deep-learning-and-graph-networks" class="headerlink" title="Relational inductive biases, deep learning, and graph networks"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnet_arXiv18/README.md" target="_blank" rel="noopener">Relational inductive biases, deep learning, and graph networks</a></h3></li></ul></li><li><h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><ul><li><h3 id="Representation-Learning-on-Graphs-Methods-and-Applications"><a href="#Representation-Learning-on-Graphs-Methods-and-Applications" class="headerlink" title="Representation Learning on Graphs: Methods and Applications"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/grl_ideb17/README.md" target="_blank" rel="noopener">Representation Learning on Graphs: Methods and Applications</a></h3></li><li><h3 id="Geometric-Deep-Learning-Going-beyond-Euclidean-data"><a href="#Geometric-Deep-Learning-Going-beyond-Euclidean-data" class="headerlink" title="Geometric Deep Learning: Going beyond Euclidean data"></a><a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gdl_isp17/README.md" target="_blank" rel="noopener">Geometric Deep Learning: Going beyond Euclidean data</a></h3></li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Graph Embeddings </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph-based deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Representation-Learning-on-Heterogeneous-Graph</title>
      <link href="/2019/12/15/representation-learning-on-heterogeneous-graph/"/>
      <url>/2019/12/15/representation-learning-on-heterogeneous-graph/</url>
      
        <content type="html"><![CDATA[<p>Representation Learning on Heterogeneous Graph including <strong>Heterogeneous Graph Embedding</strong>, <strong>Heterogeneous Graph Neural Network</strong> and <strong>Applications</strong>.</p><p>Contributed by Houye Ji. <a href="https://github.com/Jhy1993/Representation-Learning-on-Heterogeneous-Graph" target="_blank" rel="noopener">source link</a></p><h1 id="Tutorials-for-Heterogeneous-Graph"><a href="#Tutorials-for-Heterogeneous-Graph" class="headerlink" title="Tutorials for Heterogeneous Graph"></a>Tutorials for Heterogeneous Graph</h1><p>CIKM 2019 Recent Developments of Deep HIN Analysis</p><h1 id="Heterogeneous-Graph-Embedding"><a href="#Heterogeneous-Graph-Embedding" class="headerlink" title="Heterogeneous Graph Embedding"></a>Heterogeneous Graph Embedding</h1><ol><li>Yu He, Yangqiu Song, Jianxin Li, Cheng Ji, Jian Peng, Hao Peng. <strong>HeteSpaceyWalk: A Heterogeneous Spacey Random Walk for Heterogeneous Information Network Embedding</strong> CIKM 2019</li><li>Yukuo Cen, Xu Zou, Jianwei Zhang , Hongxia Yang, Jingren Zhou, Jie Tang. <strong>Representation Learning for Attributed Multiplex Heterogeneous Network.</strong> KDD 2019. <a href="https://arxiv.org/abs/1905.01669" target="_blank" rel="noopener">paper</a></li><li>Xiao Wang, Yiding Zhang, Chuan Shi. <strong>Hyperbolic Heterogeneous Information Network Embedding.</strong> AAAI 2019. <a href="http://shichuan.org/doc/65.pdf" target="_blank" rel="noopener">paper</a></li><li>Sheng Zhou, Jiajun Bu, Xin Wang, Jiawei Chen, Bingbing Hu, Defang Chen, Can Wang. <strong>HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding.</strong> ArXiv 2019. <a href="https://arxiv.org/abs/1902.01475" target="_blank" rel="noopener">paper</a></li><li>Yuanfu Lu, Chuan Shi, Linmei Hu, Zhiyuan Liu. <strong>Relation Structure-Aware Heterogeneous Information Network Embedding.</strong> AAAI 2019. <a href="http://shichuan.org/doc/63.pdf" target="_blank" rel="noopener">paper</a></li><li>Houye Ji, Chuan Shi. <strong>Attention Based Meta Path Fusion for Heterogeneous Information Network Embedding.</strong>PRICAI 2018. <a href="http://shichuan.org/doc/55.pdf" target="_blank" rel="noopener">paper</a></li><li>Hongxu Chen, Hongzhi Yin, Weiqing Wang, Hao Wang, Quoc Viet Hung Nguyen, Xue Li. <strong>PME: Projected Metric Embedding on Heterogeneous Networks for Link Prediction.</strong> KDD 2018 <a href="http://net.pku.edu.cn/daim/hongzhi.yin/papers/KDD18-Hongxu.pdf" target="_blank" rel="noopener">paper</a></li><li>Yu Shi, Qi Zhu, Fang Guo, Chao Zhang, Jiawei Han. <strong>Easing Embedding Learning by Comprehensive Transcription of Heterogeneous Information Networks</strong>. KDD 2018. <a href="https://arxiv.org/abs/1807.03490" target="_blank" rel="noopener">paper</a></li><li>Yu Shi, Huan Gui, Qi Zhu, Lance Kaplan, Jiawei Han. <strong>ASPEM：Embedding Learning by Aspects in Heterogeneous Information Networks.</strong> SDM 2018 <a href="http://yushi2.web.engr.illinois.edu/sdm18.pdf" target="_blank" rel="noopener">paper</a></li><li>Ke Tu, Peng Cui, Xiao Wang, Fei Wang, Wenwu Zhu. <strong>Structural Deep Embedding for Hyper-Networks</strong> AAAI 2018. <a href="https://arxiv.org/abs/1711.10146" target="_blank" rel="noopener">paper</a></li><li>Rana Hussein, Dingqi Yang, Philippe Cudré-Mauroux. <strong>Are Meta-Paths Necessary? Revisiting Heterogeneous Graph Embeddings.</strong> CIKM 2018. <a href="https://dl.acm.org/citation.cfm?id=3271777" target="_blank" rel="noopener">paper</a></li><li>Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu. <strong>Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction.</strong> WSDM 2018 <a href="https://arxiv.org/pdf/1712.00732" target="_blank" rel="noopener">paper</a></li><li>Meng Qu, Jian Tang, Jiawei Han. <strong>Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning.</strong> WSDM 2018.<a href="http://hanj.cs.illinois.edu/pdf/wsdm18_mqu.pdf" target="_blank" rel="noopener">paper</a></li><li>Tao-yang Fu, Wang-Chien Lee, Zhen Lei. <strong>HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning</strong> CIKM 2017 [paper](<a href="http://shichuan.org/hin/topic/Embedding/2017" target="_blank" rel="noopener">http://shichuan.org/hin/topic/Embedding/2017</a>. CIKM HIN2Vec.pdf)</li><li>Yuxiao Dong, Nitesh V. Chawla, Ananthram Swami. <strong>metapath2vec: Scalable Representation Learning for Heterogeneous Networks</strong> KDD 2017</li><li>Huan Gui, Jialu Liu, Fangbo Tao, Meng Jiang, Brandon Norick, Lance Kaplan, and Jiawei Han. <strong>Embedding Learning with Events in Heterogeneous Information Networks.</strong> TKDE 2017.</li><li>Linchuan Xu, Xiaokai Wei, Jianong Cao, Philip S. Yu. <strong>Embedding of Embedding Joint Embedding for Coupled Heterogeneous Networks.</strong> WSDM 2017.</li><li>Ludovic Dos Santos, Benjamin Piwowarski, Patrick Gallinari. <strong>Multilabel classification on heterogeneous graphs with gaussian embeddings.</strong> ECML 2016. <a href="https://link.springer.com/chapter/10.1007/978-3-319-46227-1_38" target="_blank" rel="noopener">paper</a></li><li>Jian Tang, Meng Qu, Qiaozhu Mei. <strong>PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks.</strong> KDD 2015. <a href="http://dl.acm.org/citation.cfm?id=2783307" target="_blank" rel="noopener">paper</a></li><li>Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, Thomas S. Huang. <strong>Heterogeneous Network Embedding via Deep Architectures</strong> KDD 2015 <a href="https://dl.acm.org/citation.cfm?doid=2783258.2783296" target="_blank" rel="noopener">paper</a></li><li>Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. <strong>MetaGraph2Vec: Complex Semantic Path Augmented Heterogeneous Network Embedding</strong> PAKDD 2018. <a href="https://arxiv.org/pdf/1803.02533.pdf" target="_blank" rel="noopener">paper</a></li></ol><h1 id="Heterogeneous-Graph-Neural-Network"><a href="#Heterogeneous-Graph-Neural-Network" class="headerlink" title="Heterogeneous Graph Neural Network"></a>Heterogeneous Graph Neural Network</h1><ol><li>Shaohua Fan, Junxiong Zhu, Xiaotian Han, Chuan Shi, Linmei Hu, Biyu Ma, Yongliang Li. <strong>Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation.</strong> KDD 2019. <a href="https://dl.acm.org/citation.cfm?id=3330673" target="_blank" rel="noopener">paper</a></li><li>Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, Nitesh V. Chawla. <strong>Heterogeneous Graph Neural Network.</strong> KDD 2019</li><li>Hao Peng, Jianxin Li, Qiran Gong, Yangqiu Song, Yuanxing Ning, Kunfeng Lai and Philip S. Yu <strong>Fine-grained Event Categorization with Heterogeneous Graph Convolutional.</strong> IJCAI 2019. <a href="https://arxiv.org/abs/1906.04580" target="_blank" rel="noopener">paper</a></li><li>Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Peng Cui, Philip S. Yu, Yanfang Ye.<strong>Heterogeneous Graph Attention Network.</strong> WWW 2019. <a href="https://github.com/Jhy1993/HAN" target="_blank" rel="noopener">paper</a></li><li>Yizhou Zhang, Yun Xiong, Xiangnan Kong, Shanshan Li, Jinhong Mi, Yangyong Zhu. <strong>Deep Collective Classification in Heterogeneous Information Networks.</strong> WWW 2018. <a href="https://dl.acm.org/citation.cfm?id=3186106" target="_blank" rel="noopener">paper</a></li><li>Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong Li, Le Song. <strong>Heterogeneous Graph Neural Networks for Malicious Account Detection.</strong> CIKM 2018. <a href="https://dl.acm.org/citation.cfm?id=3272010" target="_blank" rel="noopener">paper</a></li><li>Marinka Zitnik, Monica Agrawal, Jure Leskovec. <strong>Modeling polypharmacy side effects with graph convolutional networks</strong> ISMB 2018 <a href="https://arxiv.org/abs/1802.00543" target="_blank" rel="noopener">paper</a></li></ol><h1 id="Heterogeneous-Graph-Embedding-based-Application"><a href="#Heterogeneous-Graph-Embedding-based-Application" class="headerlink" title="Heterogeneous Graph Embedding based Application"></a>Heterogeneous Graph Embedding based Application</h1><ol><li>Yuyan Zheng, Chuan Shi, Xiangnan Kong, Yanfang Ye.<strong>Author Set Identification via Quasi-Clique Discovery.</strong> CIKM 2019 <a href="http://shichuan.org/doc/72.pdf" target="_blank" rel="noopener">paper</a></li><li>Yongji Wu, Defu Lian, Shuowei Jin and Enhong Chen. <strong>Graph Convolutional Networks on User Mobility Heterogeneous Graphs for Social Relationship Inference</strong> IJCAI 2019</li><li>Yanfang Ye, Shifu Hou, Lingwei Chen, Jingwei Lei, Wenqiang Wan, Jiabin Wang, Qi Xiong, and Fudong Shao .<strong>Out-of-sample Node Representation Learning for Heterogeneous Graph in</strong> <strong>Real-time Android Malware Detection</strong> IJCAI 2019</li><li>Weijian Chen, Yulong Gu, Zhaochun Ren , Xiangnan He, Hongtao Xie, Tong Guo, Dawei Yin and Yongdong Zhang.<strong>Semi-supervised User Profiling with Heterogeneous Graph Attention Networks</strong> IJCAI 2019</li><li>Yanan Xu, Yanmin Zhu, Yanyan Shen and Jiadi Yu. <strong>Learning Shared Vertex Representation in Heterogeneous Graphs with</strong> <strong>Convolutional Networks for Recommendation</strong> IJCAI 2019</li><li>Shen Wang, Zhengzhang Chen 2, Xiao Yu, Ding Li, Jingchao Ni, Lu-An Tang, Jiaping Gui, Zhichun Li, Haifeng Chen, Philip S. Yu. <strong>Heterogeneous Graph Matching Networks for Unknown Malware Detection</strong> IJCAI 2019</li><li>Linmei Hu, Tianchi Yang, Chuan Shi, Houye Ji, Xiaoli Li.<strong>Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification.</strong> EMNLP 2019. <a href="http://shichuan.org/doc/73.pdf" target="_blank" rel="noopener">paper</a></li><li>Chuan Shi, Xiaotian Han, Li Song, Xiao Wang, Senzhang Wang, Junping Du, Philip S. Yu.<strong>Deep Collaborative Filtering with Multi-Aspect Information in Heterogeneous Networks.</strong> TKDE 2019. <a href="http://shichuan.org/doc/75.pdf" target="_blank" rel="noopener">paper</a></li><li>Yiming Zhang, Yujie Fan, Wei Song, Shifu Hou, Yanfang Ye, Xin Li, Liang Zhao, Chuan Shi, Jiabin Wang, Qi Xiong.<strong>Your Style Your Identity: Leveraging Writing and Photography Styles for Drug Trafficker Identification in Darknet Markets over Attributed Heterogeneous Information Network.</strong> WWW 2019 <a href="http://shichuan.org/doc/69.pdf" target="_blank" rel="noopener">paper</a></li><li>Binbin Hu, Zhiqiang Zhang, Chuan Shi, Jun Zhou, Xiaolong Li, Yuan Qi. <strong>Cash-out User Detection based on Attributed Heterogeneous Information Network with a Hierarchical Attention Mechanism.</strong> AAAI 2019. <a href="http://shichuan.org/doc/64.pdf" target="_blank" rel="noopener">paper</a></li><li>Shaohua Fan, Chuan Shi, Xiao Wang. <strong>Abnormal Event Detection via Heterogeneous Information Network Embedding.</strong> CIKM 2018. <a href="http://shichuan.org/doc/62.pdf" target="_blank" rel="noopener">paper</a></li><li>Yujie Fan, Shifu Hou, Yiming Zhang, Yanfang Ye, Melih Abdulhayoglu <strong>Gotcha - Sly Malware! Scorpion: A Metagraph2vec Based Malware Detection System.</strong> KDD 2018.</li><li>Zemin Liu, Vicent W. Zheng, Zhou Zhao, Zhao Li, Hongxia Yang, Minghui Wu, Jing Ying. <strong>Interactive Paths Embedding for Semantic Proximity Search on Heterogeneous Graphs.</strong> KDD 2018</li><li>Vincent W. ZHENG, Mo SHA Yuchen LI, Hongxia YANG, Zhenjie ZHANG, Kian-Lee TAN. <strong>Heterogeneous embedding propagation for large-scale e-commerce user alignment</strong> ICDM 2018. <a href="https://ink.library.smu.edu.sg/sis_research/4217/" target="_blank" rel="noopener">paper</a></li><li>Chuan Shi, Binbin Hu, Wayne Xin Zhao, Philip S. Yu. <strong>Heterogeneous Information Network Embedding for Recommendation.</strong> IEEE Transactions on Knowledge and Data Engineering, 2018. <a href="http://shichuan.org/doc/48.pdf" target="_blank" rel="noopener">paper</a></li><li>Binbin Hu, Chuan Shi, Wayne Xin Zhao, Philip S. Yu. <strong>Leveraging Meta-path based Context for Top-N Recommendation with A Neural Co-Attention Model.</strong> KDD 2018. <a href="http://shichuan.org/doc/47.pdf" target="_blank" rel="noopener">paper</a></li><li>Binbin Hu, Chuan Shi, Wayne Xin Zhao, Tianchi Yang. <strong>Local and Global Information Fusion for Top-N Recommendation in Heterogeneous Information Network.</strong> CIKM 2018. <a href="http://shichuan.org/doc/61.pdf" target="_blank" rel="noopener">paper</a></li><li>Xiaotian Han, Chuan Shi, Senzhang Wang, Philip S. Yu, Li Song. <strong>Aspect-Level Deep Collaborative Filtering via Heterogeneous Information Networks.</strong> IJCAI 2018. <a href="http://shichuan.org/doc/46.pdf" target="_blank" rel="noopener">paper</a></li><li>Zemin Liu, Vicent W. Zheng, Zhou Zhao, Hongxia Yang, Kevin Chen-Chuan Chang, Minghui Wu, Jing Ying. <strong>SPE_Subgraph-augmented Path Embedding for Semantic User Search on Heterogeneous Social Network.</strong>WWW 2018.</li><li>Zemin Liu, Vincent W. Zheng, Zhou Zhao, Fanwei Zhu, Kevin Chen-Chuan Chang, Minghui Wu, Jing Ying. <strong>Distance-aware DAG Embedding for Proximity Search on Heterogeneous Graphs.</strong> AAAI 2018</li><li>Ting Chen, Yizhou Sun. <strong>Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identification</strong> WSDM 2017. <a href="https://arxiv.org/pdf/1612.02814.pdf" target="_blank" rel="noopener">paper</a></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph Neural Network </tag>
            
            <tag> Heterogeneous Graph </tag>
            
            <tag> Representation Learning </tag>
            
            <tag> Depp Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>www2019关于图表示学习相关论文阅读</title>
      <link href="/2019/12/11/www2019-graph-embeddings/"/>
      <url>/2019/12/11/www2019-graph-embeddings/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="1-MARINE"><a href="#1-MARINE" class="headerlink" title="1. MARINE"></a>1. MARINE</h1><h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><p>目前在网络embedding主要分两大分支：齐次图和多关系图。</p><p>齐次图是指结点和关系都只有一种类型，proximity是齐次图embedding最着重捕捉的属性。proximity估计节点之间的紧密度，因此合适的网络嵌入模型能为在图中彼此靠近的节点训练相似的嵌入。这个属性在社区检测、连接预测等任务中起到关键作用。</p><p>多元关系图是指结点之间的关系有多种类型。一般做法是为每一种类型使用单独的嵌入方式，然后采用逻辑运算的方法(比如相加、相乘等)将结点和关系的embedding合并。</p><p>齐次图可以看做是一种特殊的多元关系图。实验表明，现有的一些在多元关系图上的方法在齐次图上表现不一定好，可能是因为没有强调捕捉proximity信息。</p><p>此篇论文目标是提出一个图表示模型，同时适用于齐次图和多元关系图。同时，希望利用结点的属性信息。现有的齐次图中利用结点属性的方法需要有标签数据进行有监督的训练。多元关系图中的方法考虑节点属性的较少。</p><h2 id="目前方法存在的问题"><a href="#目前方法存在的问题" class="headerlink" title="目前方法存在的问题"></a>目前方法存在的问题</h2><ol><li>矩阵分解方法无法扩展到大图上，因为计算时间复杂度是O(N^2)</li><li>一些方法假设了对称关系，即$x_i^Tx_j = x_j^Tx_i$对于边$(i,j)$，无法用于有向图。</li><li>LINE中提到，基于随机游走的方法无法清晰的说明嵌入空间具体保留了图的哪些属性。</li></ol><p>此篇论文解决了上面的这些问题。</p><h2 id="设计标准"><a href="#设计标准" class="headerlink" title="设计标准"></a>设计标准</h2><p>希望能保留连接结构和节点属性信息。</p><h2 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h2><h3 id="连接结构"><a href="#连接结构" class="headerlink" title="连接结构"></a>连接结构</h3><p>给定一个关系$k$，对于边$(i,k,j)$和一个不存在的边$(i^{‘},k,j^{‘})$， 模型的得分$s(i,k,j) &gt; s(i^{‘}, k, j^{‘})$</p><h3 id="结点属性"><a href="#结点属性" class="headerlink" title="结点属性"></a>结点属性</h3><p>假设对于任意两个结点$(i,j)$，那么对于他们的嵌入表示$(x_i,x_j)$之间的距离应该与他们属性之间的距离$(\theta_i,\theta_j)$相关。</p><h3 id="得分函数设计"><a href="#得分函数设计" class="headerlink" title="得分函数设计"></a>得分函数设计</h3><p>得分函数设计为$s(i,k,j) = (x_j \odot x_i)^{\mathrm{T}}l_k + (x_j - x_i)^{\mathrm{T}}t_k$</p><p>为了获得不同的目标，定义了边关系embedding$r_k = [l_k^{\mathrm{T}}  t_k^{\mathrm{T}}]^{\mathrm{T}} \in R^{2d}$做为两个embedding向量的连接。</p><p>得分函数第一部分实际上是为了保留近似性信息的矩阵分解。</p><p>得分函数第二部分意味着，对于相同的边关系，embedding的翻译方向应该更接近。</p><p>没有设置超参数，因为$l_k,t_k$隐含了权重的作用。</p><h4 id="保留近似性"><a href="#保留近似性" class="headerlink" title="保留近似性"></a>保留近似性</h4><p>大多数先前的工作，如果基于矩阵分解，往往使用向量点积来保留近似性信息。如果基于随机游走，也是点积。使用点积$x_i^Tx_j$ 是基于一个很强的假设的：embedding空间的每一维度同等重要。对于不同关系类型的情况是不适用的，为了保证近似性质量，多关系类型情况下每一维度的重要性或者权重应该是不同的。举例说明如下：</p><p>假设embedding某个维度表示节点的“薪水”，比较两种关系”同事“和“朋友”，embedding的特定维度应该能够决定前者将比后者更为重要，这是因为两个同事比两个朋友能可能谈论薪水。为了剑魔给定不同关系下embedding每一维度的影响，论文扩展two-way矩阵分解到three-way矩阵分解，向量坐标轴分别表示主题结点$i$，边关系$k$和目标结点$j$。对于存在的边$(i,k,j) \in \wp$设置为1，不存在的边$(i,k,j)  \notin \wp$设置为0。因此，对于三元组$x_i, x_j, l_k$的embedding得分函数通过一个向量相乘表示如下：</p><p>$$\sum^d_{u=1}x_{iu}x_{ju}l_{ku} = (x_i \odot x_j)^{\mathrm{T}}l_k$$</p><p>embedding每一维度的重要性通过学习到的关系embedding$l_k$规范化，这意味着不同的embedding维度可以在确定两个节点之间是否存在关系时扮演不同的角色。那么这种设计如何满足临近性约束呢？</p><h5 id="一阶近似性"><a href="#一阶近似性" class="headerlink" title="一阶近似性"></a>一阶近似性</h5><p>给定一对结点$(i,j)$，一阶接近度意味着如果网络中有边和i和j相邻，那么在嵌入空间中$x_i$和$x_j$之间的得分就比较高.</p><h5 id="二阶近似性"><a href="#二阶近似性" class="headerlink" title="二阶近似性"></a>二阶近似性</h5><p>给定一对结点$(i,j)$，一阶接近度意味着如果结点$x_i$和$x_j$有相同的邻居，那么在嵌入空间中$x_i$和$x_j$之间的得分就比较高.</p><p>证明省略，参考原论文，不难理解。</p><h4 id="保留关系"><a href="#保留关系" class="headerlink" title="保留关系"></a>保留关系</h4><p>方法来源于[论文](Distributed Representations of Words and Phrases and their Compositionality)，在二维空间内刻画word embedding的分布。</p><p>$(x_j-x_i)^{\mathrm{T}}t_k$，$t_k$代表关系embedding向量，不对称，传统的是对称的，不能直接利用边的有向信息。</p><h3 id="节点属性"><a href="#节点属性" class="headerlink" title="节点属性"></a>节点属性</h3><p>为了利用接点属性，使其能够对学习的embedding产生影响，使用一个双射函数$\theta = f(x)$构造$\theta$和$x$之间的映射，因此embedding $x_i=f^{-1}(\theta_j)$ 在相关节点属性向量$\theta_i \approx \theta_j$时也能够靠近$x_j = f^{-1}(\theta_j)$。论文假设$\theta$的一个多元正态分布，且$双射函数f$可以任意取，最终发现简单的线性函数$f(x)=Wx+b$最好。</p><h3 id="目标函数和优化"><a href="#目标函数和优化" class="headerlink" title="目标函数和优化"></a>目标函数和优化</h3><p>前面的合起来，并且使用SGD+负采样等优化。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>三个不同的实验包括连接预测和多标签分类。8个数据集。</p><p>不同embedding结果的可视化做为另一种形式的实验。</p><p>最后分析了方法的时间复杂度，能够有效的应用于大规模图当中。</p><h3 id="baseline"><a href="#baseline" class="headerlink" title="baseline"></a>baseline</h3><p>经典模型：DeepWalk、LINE、TransE</p><p>start-of-the-art模型：SDNE、ProjE</p><p>DeepWalk、LINE和SDNE是针对齐次图的</p><p>TransE和ProjE针对多关系图</p><h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><h4 id="无监督连接预测"><a href="#无监督连接预测" class="headerlink" title="无监督连接预测"></a>无监督连接预测</h4><h4 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h4><h4 id="加节点属性"><a href="#加节点属性" class="headerlink" title="加节点属性"></a>加节点属性</h4><h4 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h4><h1 id="2-HAN"><a href="#2-HAN" class="headerlink" title="2. HAN"></a>2. HAN</h1><h2 id="基本思想-1"><a href="#基本思想-1" class="headerlink" title="基本思想"></a>基本思想</h2><p>提出基于attention的异构神经网络，包括node-level的attention和semantic-level的attention，基于meta-path来做。</p><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ol><li>第一次尝试基于attention机制用于异构图神经网络</li><li>学习包括节点级别和语义级别的attention</li><li>做了丰富的实现来证明模型的性能，并和最好的模型进行比较。对结果可视化分析，证明可解释性。</li></ol><h2 id="方法论-1"><a href="#方法论-1" class="headerlink" title="方法论"></a>方法论</h2><p>首先，提出节点级别的attention机制，以学习基于元路径的邻居的权重并对其进行聚合以获得特定语义的节点嵌入。 之后，HAN可以通过语义级别的attention来区分元路径的差异，并针对特定下游任务获得特定语义节点嵌入的最佳加权组合。</p><h3 id="node-level-attention"><a href="#node-level-attention" class="headerlink" title="node-level attention"></a>node-level attention</h3><p>首先应该明确，每个结点的邻居结点形成的元路径在针对特定下游任务学习结点embedding的时候扮演不同的角色和起到不通的重要性。因为异质性，不同类型的结点有不同的特征空间。针对每个类型的结点，构造一个特定类型的转移矩阵$M_{\phi_i}$来建模不同类型结点的特征。注意，此处基于的是节点类型，不是边类型。建模过程可以表示为$h^{‘}<em>i = M</em>{\phi_i}  \cdot h_i$，其中$h_i和h_i^{‘}$分别表示结点$i$的原始特征和建模后的特征。有了这个投影操作，节点级别的attention可以处理任意类型的结点。</p><p>接下来使用self-attention学习不同类型节点之间的权重。给定通过元路径$\Phi$联系起来的节点对$(i,j)$，结点级别的attention可以学习到权重$e_{ij}^{\Phi}$，表示结点$j$对于结点$i$的重要性。建模如下：$e_{ij}^{\Phi} = att_{node}(h_i^{‘},h_j^{‘};\Phi)$。$att_{node}$指定学习结点级别attention的深度神经网络，并且权重是在在所有路径间共享的，公式是不对称的，这对异构图来说是非常重要的特性。</p><p>得到基于节点对形成的不同元路径的重要性之后，使用softmax函数归一化得到权重系数。</p><p>然后，节点$i$的基于元路径的embedding可以通过邻居的投影特征与相应的权重系数相乘进行聚集，如下所示$z_i^{\Phi} = \sigma(\sum_{j \in N_i^{\Phi}}a_{ij}^{\Phi}\cdot h_j^{‘})$，其中$z_i^{\Phi}$是对于元路径$\Phi$中的结点$i$学到的embedding。图形化表示如下图所示：</p><p><img src="https://user-images.githubusercontent.com/8847689/70967554-80d64080-20d1-11ea-949e-541c5f0117fc.png" alt="1576559135613"></p><p>每个结点的embedding都由其邻居聚合而来。因此，给定P个元路径组成的集合，输入节点特征给结点级别attention，可以获得P组语义相关的结点embedding。</p><h3 id="semantic-level-attention"><a href="#semantic-level-attention" class="headerlink" title="semantic-level attention"></a>semantic-level attention</h3><p>取P组语义相关的从node-level embedding学习到的结点embedding做为输入，学习不同元路径中每条路径的权重：$\beta_{\Phi_0}, \beta_{\Phi_1}, \dots, \beta_{\Phi_P} = att_{sem}(Z_{\Phi_0}, Z_{\Phi_1}, \dots, Z_{\Phi_P})$。此处的$att_{sem}$定义了执行semantic-level attention的深入神经网络。</p><p>首先将其经过一个非线性转换，比如单层的MLP，然后将特定语义的embedding的重要性权重作为转换后embedding与semantic-level attention向量q的相似性进行度量。每条元路径的重要性定义为$w_{\Phi}$,计算如下：$w_{\Phi} = \frac{1}{|V|}\sum_{i \in V} q^{\mathrm{T}} \cdot\tanh(w \cdot z_i^{\Phi} + b)$,其中$w$是权重矩阵，$b$是偏移向量，$q$是semantic-level attention 向量。上面所有参数对于所有元路径和semantic-specific embedding都是共享的。得到上面的权重后，使用softmax函数进行归一化。图形化表示如上图b所示。最终的embedding由所有特定语义的embedding聚合而来。</p><h3 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h3><p>得到了node-level embedding 和semantic-level embedding，可以使用最终的embedding去针对特定的下游任务设计不通的损失函数。比如对于半监督的接点分类，可以最小化在ground-truth和预测结果之间的所有标注结点的交叉熵损失，如下所示：$L = - \sum_{l \in Y_L}Y^l \ln(C \cdot Z^l)$，其中$C$代表分类器的参数，$Y_L$代表有标签的结点集合，$Y^l$代表结点标签，$Z^l$代表结点embedding。</p><h2 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>一共用到三个数据集，包括DBLP、ACM、IMDB，详细统计信息如下表所示：</p><p><img src="https://user-images.githubusercontent.com/8847689/70985731-8d6d8f80-20f7-11ea-9cf3-90e8138e6d75.png" alt=""></p><h3 id="baseline-1"><a href="#baseline-1" class="headerlink" title="baseline"></a>baseline</h3><ul><li>DeepWalk：针对齐次图的，实验的时候忽略了图的异质性。</li><li>ESim：从多条meta-paths中捕捉语义信息，针对的是异质图。无法搜索meta-paths的权重，使用从HAN学习到的权重分配给ESim。</li><li>meta2path：基于随机游走执行meta-path，并利用skip-gram方法得到异构图的embedding。</li><li>HERec：设计了一个类型限制策略来过滤结点序列，使用skip-gram算法来得到异构图的embedding。</li><li>GCN：为齐次图设计的半监督图卷积网络。</li><li>GAT：为齐次图设计的考虑了attention机制的半监督神经网络。</li><li>HAN$_{nd}$：去掉了node-level attention，为每个邻居分配相同的权重。</li><li>HAN$_{sem}$：去掉了semantic-level attention，为每条元路径分配相同的权重。</li><li>HAN：同时使用了node-level attention和semantic-level attention。</li></ul><h3 id="下游任务-1"><a href="#下游任务-1" class="headerlink" title="下游任务"></a>下游任务</h3><h4 id="结点分类"><a href="#结点分类" class="headerlink" title="结点分类"></a>结点分类</h4><p>使用k=5的KNN分类器，重复十次计算Macro-F1和Micro-F1。</p><h4 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h4><p>使用KMeans聚类，K的值由结点类别数量决定。评价指标使用NMI和ARI。为了减轻初始值影响，重复十次。</p><h4 id="分析attention机制"><a href="#分析attention机制" class="headerlink" title="分析attention机制"></a>分析attention机制</h4><p>node-level和semantic-level，都找具体的例子</p><h4 id="可视化-1"><a href="#可视化-1" class="headerlink" title="可视化"></a>可视化</h4><p>使用t-SNE技术，对DBLP数据集中的作者结点进行可视化。</p><h4 id="参数实验"><a href="#参数实验" class="headerlink" title="参数实验"></a>参数实验</h4><p>测试模型的参数敏感性，并在ACM数据集上进行聚类，使用NMI评估。主要从以下几个方面：</p><ul><li>embedding维度，测试了16,32,64,128,256，画出折线图</li><li>semantic-level attention向量的维度：32,64,128，356,512，给出折线图</li><li>attention head(multihead attention)的数量：1,2,4,6,8，给出折线图。设置为1代表没有multihead attention。</li></ul><h1 id="3-HetGNN"><a href="#3-HetGNN" class="headerlink" title="3. HetGNN"></a>3. HetGNN</h1><p>发表于KDD19</p><h2 id="基本思想-2"><a href="#基本思想-2" class="headerlink" title="基本思想"></a>基本思想</h2><p>针对异构图，很少有工作同时考虑异构结构信息和异构内容信息。首先，引进一个带有重启策略的随机游走模型，来对每个结点强相关邻居进行固定大小的采样，然后基于节点类型进行分组。其次，设计一个由两和模块的神经网络结构来聚合这些采样邻居结点的特征信息。</p><p>第一个模块对异构内容的“深层”特征交互信息进行编码，并为每个节点生成内容embedding。第二模块聚合不同邻居结点组（类型）的内容（属性）embedding，并通过考虑不同组的影响将它们进一步组合以获得最终节点嵌入。</p><p>最后以端到端的方式使用一个graph context loss和最小batch梯度下降训练模型。</p><h2 id="GNN没有解决的一些问题"><a href="#GNN没有解决的一些问题" class="headerlink" title="GNN没有解决的一些问题"></a>GNN没有解决的一些问题</h2><ol><li><p>在异构图中，很多结点可能没有和所有类型的邻居相连接。除此之外，不同结点的邻居结点的数量也是不同的。大多数现有的GNN模型仅聚合直接（一阶）相邻节点的特征信息，并且特征传播过程可能会削弱更远的邻居的影响。此外，弱关联的邻居（“噪声”邻居）削弱了“hub”节点的嵌入生成，并且由于有限的邻居信息而无法充分表示“冷启动”节点的嵌入。如何对与异构图中每个节点的嵌入生成紧密相关的异构邻居结点进行采样，如图1（b）中的C1所示？</p><p><img src="https://user-images.githubusercontent.com/8847689/71308686-88cc1280-243a-11ea-8383-9c5d35be0fca.png" alt=""></p></li><li><p>异构图中的结点可以携带非结构化的异构内容，比如属性，文本和图像。此外，这些不同类型结点的内容是不同的。比如在图1(b)中，type-1类型结点(例如b和c)包含属性和图像，type-k类型结点包含文本和图像等。目前GNN的直接向量拼接操作或者线性转换操作不能对节点异构内容“更深”的交互关系进行建模。而且，对所有结点类型使用相同特征转换函数是不合适的，因为它们的内容彼此不同。如图1（b）中的C2所示，如何设计节点内容编码器来解决HetG中不同节点的内容异质性？</p></li><li><p>在异构图中，不同类型邻居对结点的embedding起到的贡献是不同的。当前的大多数GNN主要关注齐次图，并且不考虑节点类型的影响。如何通过考虑不同节点类型的影响来聚合异构邻居的特征信息，如图1（b）中的C3所示。</p></li></ol><h2 id="HetGNN的优势"><a href="#HetGNN的优势" class="headerlink" title="HetGNN的优势"></a>HetGNN的优势</h2><p><img src="https://user-images.githubusercontent.com/8847689/71308935-1f013800-243d-11ea-8403-ca44aca34de6.png" alt="HetGNN和其他模型对比的优势"></p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="https://user-images.githubusercontent.com/8847689/71322972-f0e92a00-2508-11ea-9e33-42a9057e351d.png" alt=""></p><h3 id="1-采样异构邻居"><a href="#1-采样异构邻居" class="headerlink" title="1. 采样异构邻居"></a>1. 采样异构邻居</h3><p>大多数图神经网络（GNN）的关键思想是聚合来自节点直接（一阶）邻居的特征信息，直接应用这些方法在异构图上会有以下问题：</p><ul><li>不能直接从不同类型的邻居中捕捉特征信息</li><li>不同的邻居数量大小带来了负面影响</li><li>对于有不同内容特征的异构邻居，GNN是不适合对此做聚合。</li></ul><p>为了解决上面的问题，论文提出了带重启的随机游走采样策略，主要包括下面两个步骤：</p><ol><li><h3 id="2-编码异构内容"><a href="#2-编码异构内容" class="headerlink" title="2. 编码异构内容"></a>2. 编码异构内容</h3></li></ol><h3 id="3-聚合异构邻居"><a href="#3-聚合异构邻居" class="headerlink" title="3. 聚合异构邻居"></a>3. 聚合异构邻居</h3><h3 id="4-建模目标函数和训练"><a href="#4-建模目标函数和训练" class="headerlink" title="4. 建模目标函数和训练"></a>4. 建模目标函数和训练</h3><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> embeddings </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph Embeddings </tag>
            
            <tag> www2019 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图表示学习的方法和应用</title>
      <link href="/2019/12/08/graph-embeddings-overreview/"/>
      <url>/2019/12/08/graph-embeddings-overreview/</url>
      
        <content type="html"><![CDATA[<h1 id="cross-graph-embedding-with-text-information-augmentation"><a href="#cross-graph-embedding-with-text-information-augmentation" class="headerlink" title="cross-graph embedding with text information augmentation"></a>cross-graph embedding with text information augmentation</h1><h2 id="encoder-decoder架构"><a href="#encoder-decoder架构" class="headerlink" title="encoder-decoder架构"></a>encoder-decoder架构</h2><p>图表示学习本质上是将图中的节点、图或者子图嵌入到一个低维空间中，不同的方法可以捕捉不同的信息，侧重点不同，都是为了更方便的利用图中的信息。</p><blockquote><p>形式化定义如下：目前的研究，图都是无向图，记为$G=(V, E)$，邻接矩阵为$A^2$，方法的目的是为了利用真实图节点属性信息(记为$X\in R^{m\times|V|}$，比如文本、元数据信息等)，映射每个节点或者子图到一个向量中。</p></blockquote><p>首先是对图中结点进行embedding，可以抽象为一个encoder-decoder架构。encoder将每个结点映射到一个低维向量，decoder从低维向量中解码结构化信息。</p><p>encoder输入是一系列结点，将结点映射到向量空间中。形式化定义为$encoder = V → R^d$</p><p>decoder输入是一系列结点向量，并从中解码用户需要的特殊图信息。比如，decoder可能是给定结点向量，预测结点之间边是否存在或者预测某个结点属于图中哪个社区(社团发现)。形式化定义为$decoder=R^d \times D^d → R^+$，将成对的结点向量映射到真实图中的”距离衡量”，即反应了原始图中的两个结点之间的距离。</p><p>因此，此架构主要包括以下四个部分：</p><ol><li>距离函数，成对，用于衡量图中两个结点有多接近</li><li>encoder</li><li>decoder</li><li>损失函数，衡量$decoder(z_i, z_j)$和真实值$s_G(v_i, v_j)$之间的差异</li></ol><h3 id="直接编码"><a href="#直接编码" class="headerlink" title="直接编码"></a>直接编码</h3><h4 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h4><p>所谓直接编码，类似于查找表，encoder直接将结点映射为嵌入向量。代表技术是矩阵分解，所有结点的向量组成的矩阵，并涉及矩阵降维和多维缩放技术。此方法核心来源于矩阵分解算法，下表比较了矩阵分解和随机游走算法在encoder-decoder框架下的表述。</p><ul><li><p>矩阵分解</p><ul><li><p>Laplacian Eigenmaps</p><p>decoder： $||z_i - z_j||_2^2$</p><p>距离函数：一般</p><p>损失函数：点积$decoder(z_i, z_j) \times s_G(v_i, v_j) $，前面乘以一个数，相当于权重</p></li><li><p>Graph Factorization</p><p>decoder: $z_i^Tz_j$，内积，因为节点之间的关系强弱与节点之间内积结果成正比，因为向量的夹角可以用内积来算。</p><p>距离函数：$A_{i,j}$(邻接矩阵中的值)</p><p>损失函数：$||decoder(z_i, z_j) - s_G(v_i, v_j)||_2^2$均方误差损失MSE</p></li><li><p>GraRep</p><p>decoder：$z_i^Tz_j$</p><p>距离函数：$A_{i,j}, A_{<em>i,j}^2\dots,A</em>{i,j}^k$，邻接矩阵的幂，能够捕捉高阶图相似性</p><p>损失函数：均方误差损失</p></li><li><p>HOPE</p><p>decoder： $z_i^Tz_j$</p><p>距离函数：一般，Jaccard距离</p><p>损失函数：均方误差损失</p></li></ul></li><li><p>随机游走</p><ul><li><p>DeepWalk</p><p>decoder: $\frac{e^{z_i^Tz_j}}{\sum_{k \in V}{e^{z_i^Tz_k}}} \approx p_{G,T}(v_j|v_i) $，没有使用固定的确定性度量方法，p是从$v_i$开始的固定长度T的随机游走情况下访问$v_j$的概率。p是随机的且不对称。</p><p>距离函数：$P_G{v_j|v_i}$，代表从$v_i$开始的固定长度随机游走情况下访问$v_j$的概率</p><p>损失函数：交叉熵损失函数$-s_G{v_i, v_j}log(decoder(z_i, z_j))$</p></li><li><p>Node2vec</p><p>decoder：$\frac{e^{z_i^Tz_j}}{\sum_{k \in V}{e^{z_i^Tz_k}}}$，和DeepWalk一样</p><p>距离函数：$P_G{v_j|v_i}$(biased)</p><p>损失函数：同DeepWalk</p></li><li><p>struct2vec(分层biased)</p></li></ul><p>总结来说，矩阵分解方法的目标是学习每个节点的嵌入式表示，使学习的嵌入式向量之间的内积近似于图的近似的某种确定性度量。而随机游走方法基于如下假设：如果某些结点在图中短距离随机游走路径上共现，那么它们的嵌入式表示存在某种近似。DW和node2vec原本时间复杂度得是O(|V|)，DW使用了hierarchical softmax来优化归一化银因子，使用二叉树结构加速计算；node2vec使用了负采样来进行优化，使用一组随机的负样本来近似归一化因子。</p></li><li><p>LINE</p><p>没有使用随机游走，却经常拿来和DW和node2vec来比较。LINE组合了两个目标函数，来捕捉一阶近似和二阶近似。一阶近似使用sigmoid函数，$decoder(z_i, z_j) = \frac{1}{1+e^{-z^T_iz_j}}$，距离函数使用的邻接矩阵衡量方法$A_{i,j}$。二阶近似相似，但是考虑了二跳邻接矩邻居，使用了类似DW的docoder。一阶和二阶目标函数都使用了来自KL散度的损失函数。</p></li></ul><h4 id="直接编码存在的问题"><a href="#直接编码存在的问题" class="headerlink" title="直接编码存在的问题"></a>直接编码存在的问题</h4><ol><li>encoder中结点之前参数无法共享(GCN？)</li><li>直接编码方法在编码过程中无法利用结点属性(比如社交网络中的用户个人资料)(已有相关论文)</li><li>直接编码方法无法泛化，即训练得到的结点embedding是基于此时固定的图的，如果图还在变化，引入了新的结点，就需要重新训练。(已有相关论文)</li></ol><h3 id="邻居自动编码"><a href="#邻居自动编码" class="headerlink" title="邻居自动编码"></a>邻居自动编码</h3><h4 id="主要方法-1"><a href="#主要方法-1" class="headerlink" title="主要方法"></a>主要方法</h4><p>Deep Neural Graph Representations(DNGR)和Structural Deep Network Embeddings(SDNE)提出，与直接编码方法不同，他们在编码器中直接合并了图的结构信息。基本思想是使用了自动编码器：抽取高维邻居向量，然后压缩为低纬向量(使用自动编码器)。每个结点都和一个邻居向量相关联，对应矩阵中的一行。encoder和decoder都是多层神经网络，并且encoder的每一层都会减少它的输入向量的维度，decoder的每一层都会增加它的输入的维度。DNGR和SDNE有着非常大的不同。</p><p>在构建邻居向量$s_i$上，DNGR根据随机游动中同时出现的两个节点的点向互信息来定义si，类似于DeepWalk和node2vec。SDNE则是简单的设定$s_i \triangleq A_i$，例如等于$v_i$的邻接向量。同时SDNE也使用了包含拉普拉斯特征图目标函数的自动编码目标函数。因为encoder依赖的输入$s_i$包含了节点$v_i$的local graph neighborhood information，所以可以直接将结结点邻居的结构化信息编码到encoder中，但是直接编码方法就行，因为它仅依赖于结点的id。</p><h4 id="邻居自动编码存在的问题"><a href="#邻居自动编码存在的问题" class="headerlink" title="邻居自动编码存在的问题"></a>邻居自动编码存在的问题</h4><p>输入的维度固定为$V$，计算很慢，尤其是图很大的时候。除此之外，自动编码器的结构和大小是固定的，不适合动态演进的图，也不能在图之间泛化。</p><h3 id="邻居聚合和卷积编码器"><a href="#邻居聚合和卷积编码器" class="headerlink" title="邻居聚合和卷积编码器"></a>邻居聚合和卷积编码器</h3><p>直接编码和自动编码器有固定的缺陷，为了仅依赖结点的局部邻居而不是整个图，最近的方法开始尝试通过聚集来自其本地结点邻域的信息来生成结点的嵌入。如下图所示</p><p><img src="/media/tenyun/Documents/wds/%E5%8D%9A%E5%AE%A2/imgs/1575809069682.png" alt="1575809069682"></p><p>不同于其他方法，这些邻域聚合算法依赖于结点特征和属性来生成向量，比如文本信息、用户属性等。加入没有给结点属性信息，还可以使用一些图的统计信息，比如结点的度等。之所以称为卷积也是因为其将结点表示为周围邻居的函数，类似图像中中心环绕的卷积核。</p><p>邻域聚合的方法一般通过迭代或者递归的方式来训练结点的向量表示。</p><ol><li>结点embedding初始化为结点的属性</li><li>encoder算法的每次迭代，结点都使用对向量进行聚合运算的函数来聚合它的邻居的embeddings。此时，每个结点都被分配一个新的embedding，等于其聚合的邻域embedding与其上次迭代中的先前embedding相结合。</li><li>组合的embedding送入深层神经网络。</li><li>重复上述过程K次。</li></ol><p>随着过程的迭代进行，结点embedding包含从图的越来越远的地方聚合的信息，但是embedding维度不变。</p><h2 id="结合特定下游任务的有监督方法"><a href="#结合特定下游任务的有监督方法" class="headerlink" title="结合特定下游任务的有监督方法"></a>结合特定下游任务的有监督方法</h2><p>基本的encoder-decoder是无监督的。但是有些算法比如上面提到的邻域聚合算法也可以结合特定任务进行有监督的学习结点embedding，常用的任务一般是结点分类，下面以二分类举例，多分类可以看做多个二分类。</p><blockquote><p>假设二分类标签为$y_i\in Z$，将embedding向量$z_i$喂给逻辑斯蒂或者sigmoid函数，从而将结点映射到它们的标签，记为$\hat {y_i} = \sigma(z^T_i\theta)$，其中$\theta$代表可训练的参数向量。</p></blockquote><p>基于上面的定义，可以简单的通过交叉熵损失函数来预测每个标签的概率，如下所示：</p><p>$$\ell = \sum_{v_i \in V}{y_i\log(\sigma(encoder(v_i)^T\theta)) + (1-y_i)\log(1-\theta(encoder(v_i^T\theta)))}$$</p><p>可以通过反向传播来进行参数优化，可以发现，特定任务有监督的方法可以完全通过decoder计算出损失，也可以和decoder的损失合在一起。</p><h2 id="多模态图"><a href="#多模态图" class="headerlink" title="多模态图"></a>多模态图</h2><p>前面说的图都是简单的、无向图，也有很多复杂图。</p><h3 id="不同的结点和边的类型"><a href="#不同的结点和边的类型" class="headerlink" title="不同的结点和边的类型"></a>不同的结点和边的类型</h3><p>许多图的节点和边都有不同的类型。一般的解决方法如下：</p><ol><li>针对不同类型的结点使用不通的encoder</li><li>使用针对特定类型的参数来扩展pairwise decoder</li></ol><p>比如针对不同的边的类型，以前的标准内积decoder形式($z_i^Tz_j \approx A_{i,j} $)可以使用一个双线性的形式取代$$decoder(z_i, z_j) = z^TA_{\tau}z$$，其中$\tau$代表一个特殊的边的类型，同事$A_{\tau}$是一个针对$\tau$类型的边的可学习的参数。这在具有大量不通类型的边的知识图谱中非常有用，比如知识补全，预测缺失的边等。</p><p>Dong et.提出在异构图上基于随机游走采样的策略，其随机游走的路径限定在特定类型结点之间。这种思想可以将前面的很多方法用在异构图中，可以使用特定类型的encoder和decoder。</p><h3 id="分层结点嵌入"><a href="#分层结点嵌入" class="headerlink" title="分层结点嵌入"></a>分层结点嵌入</h3><p>有的图包含许多层，层与层之间包括一些复制的结点。例如，在源自不同组织（例如，大脑或肝脏组织）的蛋白质-蛋白质相互作用网络中，某些蛋白质可以在多个组织中存在。如何在不同层之间共享信息很重要，这样节点在某一层的嵌入可以通过其在其他层的嵌入来得到。OhmNet改进了node2vec，加了个关于层的惩罚正则项。假设有个结点$v_i$属于两个不通的层$G1,G2$，损失函数可以修改如下</p><p>$$\ell(v_i)^{\prime} = \ell(v_i) + \lambda||z_i^{G_1} - z_i^{G_2}||$$</p><p>其中$ \ell $定义了结点的损失，$\lambda$定义了正则化因子，$z_i^{G_1}$和$z_i^{G_2}$分别代表$v_i$在两个不同层的embedding。</p><h2 id="不同结构角色的图嵌入"><a href="#不同结构角色的图嵌入" class="headerlink" title="不同结构角色的图嵌入"></a>不同结构角色的图嵌入</h2><p>node2vec通过biased的随机游走使得模型能够更好的捕捉结构信息。</p><p>struct2vec从原始图来生成一系列加权的辅助图，辅助图能够捕捉不同结点k跳邻居的结构相似性。假设$R_k(v_i)$代表从$v_i$开始k跳的结点的度的有序序列，边的权重为$w_k(v_i, v_j)$，辅助图$G_k^ \prime, k= 1,2,\dots$ </p><p>$$w_k(v_i, v_j) = w_{k-1}(v_i, v_j) + d(R_k(v_i), R_k(v_j))$$</p><p>其中$w_0(v_i, v_j) = 0$, $d(R_k(v), R_k(v_j))$代表有序度序列$R_k(v_i)$和$R_k(v_j)$之间的距离。计算出权重以后，struct2vec使用biased的随机游走得到结点序列，然后使用node2vec的优化算法得到结点向量表示。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> graph-embeddings </category>
          
      </categories>
      
      
        <tags>
            
            <tag> graph-embeddings </tag>
            
            <tag> 综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go get fail的解决办法</title>
      <link href="/2019/12/05/go-get-error/"/>
      <url>/2019/12/05/go-get-error/</url>
      
        <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>当我们使用 <code>go get</code>、<code>go install</code>、<code>go mod</code> 等命令时，会自动下载相应的包或依赖包。但由于众所周知的原因，类似于 <code>golang.org/x/...</code> 的包会出现下载失败的情况。如下所示：</p><pre class=" language-go"><code class="language-go"><span class="token keyword">go</span> get <span class="token operator">-</span>u golang<span class="token punctuation">.</span>org<span class="token operator">/</span>x<span class="token operator">/</span>sys  <span class="token keyword">go</span> get golang<span class="token punctuation">.</span>org<span class="token operator">/</span>x<span class="token operator">/</span>sys<span class="token punctuation">:</span> unrecognized <span class="token keyword">import</span> path <span class="token string">"golang.org/x/sys"</span> <span class="token punctuation">(</span>https fetch<span class="token punctuation">:</span> Get https<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>golang<span class="token punctuation">.</span>org<span class="token operator">/</span>x<span class="token operator">/</span>sys?<span class="token keyword">go</span><span class="token operator">-</span>get<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">:</span> dial tcp <span class="token number">216.239</span><span class="token punctuation">.</span><span class="token number">37.1</span><span class="token punctuation">:</span><span class="token number">443</span><span class="token punctuation">:</span> i<span class="token operator">/</span>o timeout<span class="token punctuation">)</span></code></pre><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><p>那我们该如何解决问题呢？</p><h3 id="手动下载"><a href="#手动下载" class="headerlink" title="手动下载"></a>手动下载</h3><p>我们常见的 <code>golang.org/x/...</code> 包，一般在 GitHub 上都有官方的镜像仓库对应。比如 <code>golang.org/x/text</code>对应 <code>github.com/golang/text</code>。所以，我们可以手动下载或 clone 对应的 GitHub 仓库到指定的目录下。</p><pre class=" language-go"><code class="language-go">mkdir $GOPATH<span class="token operator">/</span>src<span class="token operator">/</span>golang<span class="token punctuation">.</span>org<span class="token operator">/</span>x cd $GOPATH<span class="token operator">/</span>src<span class="token operator">/</span>golang<span class="token punctuation">.</span>org<span class="token operator">/</span>x git clone git@github<span class="token punctuation">.</span>com<span class="token punctuation">:</span>golang<span class="token operator">/</span>text<span class="token punctuation">.</span>git rm <span class="token operator">-</span>rf text<span class="token operator">/</span><span class="token punctuation">.</span>git`</code></pre><p>当如果需要指定版本的时候，该方法就无解了，因为 GitHub 上的镜像仓库多数都没有 tag。并且，手动嘛，程序员怎么能干呢，尤其是依赖的依赖，太多了。</p><h3 id="设置代理"><a href="#设置代理" class="headerlink" title="设置代理"></a>设置代理</h3><p>如果你<strong>有代理</strong>，那么可以设置对应的环境变量：</p><pre class=" language-go"><code class="language-go">export http_proxy<span class="token operator">=</span>http<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>proxyAddress<span class="token punctuation">:</span>port export https_proxy<span class="token operator">=</span>http<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>proxyAddress<span class="token punctuation">:</span>port</code></pre><p>或者，直接用 <code>all_proxy</code>：</p><pre class=" language-go"><code class="language-go">export all_proxy<span class="token operator">=</span>http<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>proxyAddress<span class="token punctuation">:</span>port</code></pre><h3 id="go-mod-replace"><a href="#go-mod-replace" class="headerlink" title="go mod replace"></a>go mod replace</h3><p>从 Go 1.11 版本开始，新增支持了 <code>go modules</code> 用于解决包依赖管理问题。该工具提供了 <code>replace</code>，就是为了解决包的别名问题，也能替我们解决 <code>golang.org/x</code> 无法下载的的问题。</p><p><code>go module</code> 被集成到原生的 <code>go mod</code> 命令中，但是如果你的代码库在 <code>$GOPATH</code> 中，<code>module</code> 功能是默认不会开启的，想要开启也非常简单，通过一个环境变量即可开启 <code>export GO111MODULE=on</code>。</p><p>以下为参考示例：</p><pre class=" language-go"><code class="language-go">module example<span class="token punctuation">.</span>com<span class="token operator">/</span>hello  <span class="token function">require</span> <span class="token punctuation">(</span>         golang<span class="token punctuation">.</span>org<span class="token operator">/</span>x<span class="token operator">/</span>text v0<span class="token number">.3</span><span class="token punctuation">.</span><span class="token number">0</span> <span class="token punctuation">)</span>  <span class="token function">replace</span> <span class="token punctuation">(</span>         golang<span class="token punctuation">.</span>org<span class="token operator">/</span>x<span class="token operator">/</span>text <span class="token operator">=</span><span class="token operator">></span> github<span class="token punctuation">.</span>com<span class="token operator">/</span>golang<span class="token operator">/</span>text v0<span class="token number">.3</span><span class="token punctuation">.</span><span class="token number">0</span> <span class="token punctuation">)</span></code></pre><p>类似的还有 <a href="https://github.com/Masterminds/glide" target="_blank" rel="noopener">glide</a>、<a href="https://github.com/gpmgo/gopm" target="_blank" rel="noopener">gopm</a> 等这类第三方包管理工具，都不同方式的解决方案提供给我们。</p><h3 id="GOPROXY-环境变量"><a href="#GOPROXY-环境变量" class="headerlink" title="GOPROXY 环境变量"></a>GOPROXY 环境变量</h3><p>终于到了本文的终极大杀器 —— <strong>GOPROXY</strong>。</p><p>我们知道从 <code>Go 1.11</code> 版本开始，官方支持了 <code>go module</code> 包依赖管理工具。</p><p>其实还新增了 <code>GOPROXY</code> 环境变量。如果设置了该变量，下载源代码时将会通过这个环境变量设置的代理地址，而不再是以前的直接从代码库下载。这无疑对我等无法科学上网的开发良民来说是最大的福音。</p><p>更可喜的是，<a href="https://github.com/goproxyio/goproxy" target="_blank" rel="noopener">goproxy.io</a> 这个开源项目帮我们实现好了我们想要的。该项目允许开发者一键构建自己的 <code>GOPROXY</code> 代理服务。同时，也提供了公用的代理服务 <code>https://goproxy.io</code>，我们只需设置该环境变量即可正常下载被墙的源码包了：</p><pre class=" language-go"><code class="language-go">export GOPROXY<span class="token operator">=</span>https<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>goproxy<span class="token punctuation">.</span>io</code></pre><p>不过，<strong>需要依赖于 go module 功能</strong>。可通过 <code>export GO111MODULE=on</code> 开启 MODULE。</p><p>如果项目不在 <code>GOPATH</code> 中，则无法使用 <code>go get ...</code>，但可以使用 <code>go mod ...</code> 相关命令。</p><p>也可以通过置空这个环境变量来关闭，<code>export GOPROXY=</code>。</p><p>对于 Windows 用户，可以在 <code>PowerShell</code> 中设置：</p><pre class=" language-go"><code class="language-go">$env<span class="token punctuation">:</span>GOPROXY <span class="token operator">=</span> <span class="token string">"https://goproxy.io"</span></code></pre><p>最后，我们当然推荐使用 <code>GOPROXY</code> 这个环境变量的解决方式，前提是 <strong>Go version &gt;= 1.11</strong>。</p><p>最后的最后，七牛也出了个国内代理 <a href="https://github.com/goproxy/goproxy.cn" target="_blank" rel="noopener">goproxy.cn</a> 方便国内用户更快的访问不能访问的包，真是良心。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://mp.weixin.qq.com/s/COethtOaiygsYev-kkCc4A" target="_blank" rel="noopener">goproxy.io for Go modules</a></li><li><a href="https://goproxy.io/" target="_blank" rel="noopener">goproxy.io</a></li><li><a href="https://github.com/goproxy/goproxy.cn" target="_blank" rel="noopener">goproxy.cn</a></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
            <tag> goproxy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vim-base-control-2</title>
      <link href="/2019/12/02/vim-base-control-2/"/>
      <url>/2019/12/02/vim-base-control-2/</url>
      
        <content type="html"><![CDATA[<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><pre><code>:e filename     在编辑器中打开一个文件:w              保存文件:q              退出vim:q!             退出但不保存:x              写文件(如果有做修改)并退出:sav filename   保存为.               在正常模式中重复执行上一个变更5.              重复五次</code></pre><h3 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h3><pre><code>k or Up Arrow   上移一行j or Down Arrow 下移一行e               移动到单词末尾b               移动到单词开头0               移动到行首G               移动到文件末尾gg              移动到文件开头L               移动到屏幕底:59             移动到59行20|             移动到第20列%               移动到匹配的括号[[              到函数头[{              到块开始位置</code></pre><h3 id="剪切，复制和粘贴"><a href="#剪切，复制和粘贴" class="headerlink" title="剪切，复制和粘贴"></a>剪切，复制和粘贴</h3><pre><code>y   拷贝选中部分到剪贴板p   粘贴剪贴板中内容dd  剪切当前行yy  拷贝当前行y$  拷贝到行尾D   剪切到行尾</code></pre><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><pre><code>/word           从开头到结尾搜索单词word?word           从结尾到卡头*               搜索光标下单词/\cstring       搜索string或STRING, 大小写不敏感/jo[ha]n        搜索john 或 joan/\&lt; the         搜索以the开头的，the, theatre 或 then/the\&gt;          搜索以the结尾的，the 或 breathe/\&lt; the\&gt;       搜索the/\&lt; ¦.\&gt;        搜索所有含有四个字母的/\/             搜索fred 但不是alfred 或 frederick/fred\|joe      搜索fred 或 joe/\&lt;\d\d\d\d\&gt;   搜索仅有四个数字的/^\n\{3}        搜索连续3个空行的:bufdo /searchstr/  在所有打开buf中搜索bufdo %s/something/somethingelse/g  在所有打开buf中搜索并替换</code></pre><h3 id="替换"><a href="#替换" class="headerlink" title="替换"></a>替换</h3><pre><code>:%s/old/new/g           将所有出现的old替换为new:%s/onward/forward/gi   将所有onward替换为forward，大小写不敏感:%s/old/new/gc          替换前确认:2,35s/old/new/g        将第2行到第35行之间的old替换为new:5,$s/old/new/g         将第5行到文件结尾的old替换为new:%s/^/hello/g           在每一行开头加入hello:%s/$/Harry/g           在每一行结尾加入Harry:%s/ *$//g              删除每行末尾无用空格:g/string/d             删除所有包含string的行:v/string/d             删除所有不包含string的行:s/Bill/Steve/          替换当前行第一个Bill为Steve:s/Bill/Steve/g         替换当前行中所有Bill:%s/Bill/Steve/g        替换文件中所有Bill:%s/^M//g               删掉DOS保存文件中(^M):%s/\r/\r/g             Transform DOS carriage returns in returns:%s#&lt;[^&gt;]\+&gt;##g         删除html标签但是保留文本:%s/^\(.*\)\n\1$/\1/    删除所有连续出现过两次的行，保留一行Ctrl+a                  递增光标下的数字Ctrl+x                  递减光标下的数字ggVGg?                  文本转换为 Rot13</code></pre><h3 id="大小写"><a href="#大小写" class="headerlink" title="大小写"></a>大小写</h3><pre><code>Vu                  整行小写VU                  整行大写g~~                 整行大小写反转vEU                 单词转为大写vE~                 单词大小写反转ggguG               所有文本小写gggUG               所有文本大写:set ignorecase     搜索中忽略大小写:set smartcase      搜索中忽略大小写，除非搜索词中存在大小写字母:%s/\&lt;./\u&amp;/g       将所有单词首字母大写:%s/\&lt;./\l&amp;/g       将所有单词首字母小写:%s/.*/\u&amp;          将每行第一个字母大写:%s/.*/\l&amp;          将每行第一个字母小写</code></pre><h3 id="读写文件"><a href="#读写文件" class="headerlink" title="读写文件"></a>读写文件</h3><pre><code>:1,10 w outfile     1到10行内容写到outfile:1,10 w &gt;&gt; outfile  1到10行内容追加到outfile:r infile           插入文件内容:23r infile         插入文件23行的内容</code></pre><h3 id="文件浏览器"><a href="#文件浏览器" class="headerlink" title="文件浏览器"></a>文件浏览器</h3><pre><code>:e .                打开完整文件浏览器:Sex                切分窗口，打开文件浏览器:Sex!               同上，垂直切分:browse e           图像化文件浏览器:ls                 列出buffers:cd ..              移到上一层目录:args               列出文件:args *.php         打开文件列表:grep expression *.php  返回包含expression的php文件列表gf                  打开光标下文件名对应的文件</code></pre><h3 id="和Unix交互"><a href="#和Unix交互" class="headerlink" title="和Unix交互"></a>和Unix交互</h3><pre><code>:!pwd               执行pwd命令，返回结果!!pwd               执行命令并插入结果到文件中:sh                 临时返回unix$exit               从unix中返回vim</code></pre><h3 id="对齐"><a href="#对齐" class="headerlink" title="对齐"></a>对齐</h3><pre><code>:%!fmt              所有行对齐!}fmt               当前位置所有行对齐5!!fmt              后五行对齐</code></pre><h3 id="Tabs和Windows"><a href="#Tabs和Windows" class="headerlink" title="Tabs和Windows"></a>Tabs和Windows</h3><pre><code>:tabnew             创建一个新的tabgt                  展示下一个tab:tabfirst           展示第一个tab:tablast            展示最后一个tag:tabm n(position)   重排tab:tabdo %s/foo/bar/g 在所有tab中执行一个命令:tab ball           将所有打开文件放入tab中:new abc.txt        在新window中编辑abc.txt</code></pre><h3 id="窗口分屏"><a href="#窗口分屏" class="headerlink" title="窗口分屏"></a>窗口分屏</h3><pre><code>:e filename         在当前窗口中编辑文件:split filename     切分当前窗口并打开文件(缩写 :sp filename)ctrl-w up arrow     移到上一个文件ctrl-w ctrl-w       移到下一个窗口ctrl-w_             当前窗口垂直最大化ctrl-w|             当前窗口水平最大化ctrl-w=             所有窗口等大小10 ctrl-w+          当前窗口增加10行:vsplit file        竖直切分窗口:sview file         同:split, 只读模式:hide               关闭当前窗口:­nly               关闭出了当前窗口之外的所有窗口:b 2                打开2号窗口</code></pre><h3 id="自动补全"><a href="#自动补全" class="headerlink" title="自动补全"></a>自动补全</h3><pre><code>Ctrl+n Ctrl+p (插入模式)  补全单词Ctrl+x Ctrl+l           补全行:set dictionary=dict    定义dict为dictionnaryCtrl+x Ctrl+k           用字典中内容补全</code></pre><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><pre><code>m {a-z}                 在当前位置做标签{a-z}' {a-z}                 移动到标签位置''                      移动到上一个位置</code></pre><h3 id="缩写"><a href="#缩写" class="headerlink" title="缩写"></a>缩写</h3><pre><code>:ab mail mail@provider.org      定义mail 作为 mail@provider.org的缩写</code></pre><h3 id="文本缩进"><a href="#文本缩进" class="headerlink" title="文本缩进"></a>文本缩进</h3><pre><code>:set autoindent         打开自动缩进:set smartindent        打开自动智能缩进:set shiftwidth=4       缩进设为4个空格ctrl-t, ctrl-d          插入模式中缩进/去缩进&gt;&gt;                      缩进&lt;&lt;                      去缩进=%                      缩进括号中的代码1GVG=                   缩进整个文件</code></pre><h3 id="语法高亮"><a href="#语法高亮" class="headerlink" title="语法高亮"></a>语法高亮</h3><pre><code>:syntax on              打开语法高亮:syntax off             关闭语法高亮:set syntax=perl        强制语法高亮</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> VIM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VIM </tag>
            
            <tag> vim命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vim-base-control</title>
      <link href="/2019/12/02/vim-base-control/"/>
      <url>/2019/12/02/vim-base-control/</url>
      
        <content type="html"><![CDATA[<h2 id="命令历史"><a href="#命令历史" class="headerlink" title="命令历史"></a>命令历史</h2><p>以<code>:</code>和<code>/</code>开头的命令都有历史纪录，可以首先键入:或/然后按上下箭头来选择某个历史命令。</p><h2 id="启动vim"><a href="#启动vim" class="headerlink" title="启动vim"></a>启动vim</h2><p>在命令行窗口中输入以下命令即可</p><ul><li><code>vim</code> 直接启动vim</li><li><code>vim filename</code> 打开vim并创建名为filename的文件</li></ul><h2 id="文件命令"><a href="#文件命令" class="headerlink" title="文件命令"></a>文件命令</h2><ul><li>打开单个文件<code>vim file</code> </li><li>同时打开多个文件<code>vim file1 file2 file3 ...</code> </li><li>在vim窗口中打开一个新文件<code>:open file</code> </li><li>在新窗口中打开文件<code>:split file</code> </li><li>切换到下一个文件<code>:bn</code> </li><li>切换到上一个文件<code>:bp</code> </li><li>查看当前打开的文件列表，当前正在编辑的文件会用[]括起来。<code>:args</code> </li><li>打开远程文件，比如ftp或者share folder<code>:e ftp://192.168.10.76/abc.txt</code>或者<code>:e \\qadrive\test\1.txt</code> </li></ul><h2 id="vim的模式"><a href="#vim的模式" class="headerlink" title="vim的模式"></a>vim的模式</h2><ul><li>正常模式（按<code>Esc</code>或Ctrl+[进入） 左下角显示文件名或为空</li><li>插入模式（按<code>i</code>键进入） 左下角显示–INSERT–</li><li>可视模式（不） 左下角显示–VISUAL–</li><li>导航命令</li></ul><h3 id="移动命令"><a href="#移动命令" class="headerlink" title="移动命令"></a>移动命令</h3><ul><li><code>^</code>:移动光标到行首；</li><li><code>$</code>:移动光标到行尾；</li><li><code>ctrl-b</code>:类似于键盘上的”PgUp”(b为backword)</li><li><code>ctrl-f</code>：类似于键盘上的”PgDn”(f为forword)</li><li><code>G</code>：移动到末行；</li><li><code>1G</code>：移动到首行；</li><li><code>50G</code>：移动到50行；</li><li><code>H</code>：移动到当前窗口的首行；</li><li><code>M</code>：移动到当前窗口的中间位置；</li><li><code>L</code>：移动光标到当前窗口的最后一行；</li><li><code>w</code>:光标移动到下一个单词的词首；注：对于中文，连续的多个汉字作为一个word。</li><li><code>2w</code>:重复执行w操作2次；</li><li><code>e</code>:光标移动到下一个单词的词尾；</li><li><code>5e</code>:重复执行e操作5次；</li><li><code>b</code>：向前移动光标，移动到前一个单词的词首；</li></ul><h4 id="句字-sentences-直接移动操作："><a href="#句字-sentences-直接移动操作：" class="headerlink" title="句字(sentences)直接移动操作："></a>句字(sentences)直接移动操作：</h4><ul><li><code>)</code>:光标移动到下一句；</li><li><code>(</code>:光标移动到上一句；</li><li><code>3)</code>:光标移动到向下3句</li></ul><h4 id="段落（paragraphs）直接移动操作："><a href="#段落（paragraphs）直接移动操作：" class="headerlink" title="段落（paragraphs）直接移动操作："></a>段落（paragraphs）直接移动操作：</h4><ul><li><code>{</code>:向上移动一个段落；</li><li><code>}</code>:向下移动一个段落</li><li><code>3}</code>:向下移动3个段落</li></ul><p>更多操作在vim Normal模式下输入 <code>:help cursor-motions</code></p><p><img src="https:////upload-images.jianshu.io/upload_images/1464813-64f174231649a882?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="img"></p><p>vim</p><p><a href="https://link.jianshu.com?t=http://blog.csdn.net/richerg85/article/details/17636827" target="_blank" rel="noopener">http://blog.csdn.net/richerg85/article/details/17636827</a></p><h3 id="vim-快速选中并复制粘贴替换一个单词"><a href="#vim-快速选中并复制粘贴替换一个单词" class="headerlink" title="vim 快速选中并复制粘贴替换一个单词"></a>vim 快速选中并复制粘贴替换一个单词</h3><ol><li>光标移动到aaa的开头，按 v 按e 按y</li><li>光标移动到bbb的开头，按 v 按e 按p<br> 也就说，快速选中一个单词，按v按e即可。</li></ol><ul><li>复制一个单词: <code>yaw</code> </li><li>复制一行: <code>yim</code> 光标在中间</li></ul><h3 id="插入命令"><a href="#插入命令" class="headerlink" title="插入命令"></a>插入命令</h3><ul><li><code>i</code> 在当前位置生前插入</li><li><code>I</code> 在当前行首插入</li><li><code>a</code> 在当前位置后插入</li><li><code>A</code> 在当前行尾插入</li><li><code>o</code> 在当前行之后插入一行</li><li><code>O</code> 在当前行之前插入一行</li></ul><h3 id="查找命令"><a href="#查找命令" class="headerlink" title="查找命令"></a>查找命令</h3><ul><li><code>/text</code>　　查找text，按n健查找下一个，按N健查找前一个。</li><li><code>?text</code>　　查找text，反向查找，按n健查找下一个，按N健查找前一个。</li><li>vim中有一些特殊字符在查找时需要转义　　<code>.*[]^%/?~$</code> </li><li><code>:set ignorecase</code>　　忽略大小写的查找</li><li><code>:set noignorecase</code>　　不忽略大小写的查找</li><li>查找很长的词，如果一个词很长，键入麻烦，可以将光标移动到该词上，按<code>*</code>或<code>#</code>键即可以该单词进行搜索，相当于/搜索。而#命令相当于?搜索。</li><li><code>:set hlsearch</code>　　高亮搜索结果，所有结果都高亮显示，而不是只显示一个匹配。</li><li><code>:set nohlsearch</code>　　关闭高亮搜索显示</li><li><code>:nohlsearch</code>　　关闭当前的高亮显示，如果再次搜索或者按下n或N键，则会再次高亮。</li><li><code>:set incsearch</code>　　逐步搜索模式，对当前键入的字符进行搜索而不必等待键入完成。</li><li><code>:set wrapscan</code>　　重新搜索，在搜索到文件头或尾时，返回继续搜索，默认开启。</li></ul><h3 id="替换命令"><a href="#替换命令" class="headerlink" title="替换命令"></a>替换命令</h3><ul><li><code>ra</code> 将当前字符替换为a，当期字符即光标所在字符。</li><li><code>s/old/new/</code> 用old替换new，替换当前行的第一个匹配</li><li><code>s/old/new/g</code> 用old替换new，替换当前行的所有匹配</li><li><code>%s/old/new/</code> 用old替换new，替换所有行的第一个匹配</li><li><code>%s/old/new/g</code> 用old替换new，替换整个文件的所有匹配</li><li><code>:10,20 s/^/ /g</code> 在第10行知第20行每行前面加四个空格，用于缩进。</li><li><code>ddp</code> 交换光标所在行和其下紧邻的一行。</li></ul><h3 id="移动命令-1"><a href="#移动命令-1" class="headerlink" title="移动命令"></a>移动命令</h3><ul><li><code>h</code> 左移一个字符</li><li><code>l</code> 右移一个字符，这个命令很少用，一般用w代替。</li><li><code>k</code> 上移一个字符</li><li><code>j</code> 下移一个字符</li><li>以上四个命令可以配合数字使用，比如<code>20j</code>就是向下移动20行，<code>5h</code>就是向左移动5个字符，在Vim中，很多命令都可以配合数字使用，比如删除10个字符<code>10x</code>，在当前位置后插入3个！，<code>3a！&lt;Esc&gt;</code>，这里的Esc是必须的，否则命令不生效。</li><li><code>w</code> 向前移动一个单词（光标停在单词首部），如果已到行尾，则转至下一行行首。此命令快，可以代替l命令。</li><li><code>b</code> 向后移动一个单词 <code>2b</code> 向后移动2个单词</li><li><code>e</code>，同w，只不过是光标停在单词尾部</li><li><code>ge</code>，同b，光标停在单词尾部。</li><li><code>^</code> 移动到本行第一个非空白字符上。</li><li><code>0</code>（数字0）移动到本行第一个字符上，</li><li><code>&lt;HOME&gt;</code> 移动到本行第一个字符。同0健。</li><li><code>$</code> 移动到行尾 <code>3$</code> 移动到下面3行的行尾</li><li><code>gg</code> 移动到文件头。 = [[</li><li><code>G</code>（shift + g） 移动到文件尾。 = ]]</li><li><code>f</code>（find）命令也可以用于移动，<code>fx</code>将找到光标后第一个为x的字符，<code>3fd</code>将找到第三个为d的字符。</li><li><code>F</code> 同f，反向查找。</li><li>跳到指定行，<code>:n</code>，回车，比如跳到240行就是 :240回车。另一个方法是行号+G，比如230G跳到230行。</li><li><code>Ctrl + e</code> 向下滚动一行</li><li><code>Ctrl + y</code> 向上滚动一行</li><li><code>Ctrl + d</code> 向下滚动半屏</li><li><code>Ctrl + u</code> 向上滚动半屏</li><li><code>Ctrl + f</code> 向下滚动一屏</li><li><code>Ctrl + b</code> 向上滚动一屏</li></ul><h3 id="撤销和重做"><a href="#撤销和重做" class="headerlink" title="撤销和重做"></a>撤销和重做</h3><ul><li><code>u</code> 撤销（Undo）</li><li><code>U</code> 撤销对整行的操作</li><li><code>Ctrl + r</code> 重做（Redo），即撤销的撤销。</li></ul><h3 id="删除命令"><a href="#删除命令" class="headerlink" title="删除命令"></a>删除命令</h3><ul><li><code>x</code> 删除当前字符</li><li><code>3x</code> 删除当前光标开始向后三个字符</li><li><code>X</code> 删除当前字符的前一个字符。X=dh</li><li><code>dl</code> 删除当前字符， dl=x</li><li><code>dh</code> 删除前一个字符</li><li><code>dd</code> 删除当前行</li><li><code>dj</code> 删除上一行</li><li><code>dk</code> 删除下一行</li><li><code>10d</code> 删除当前行开始的10行。</li><li><code>D</code> 删除当前字符至行尾。D=d$</li><li><code>d$</code> 删除当前字符之后的所有字符（本行）</li><li><code>kdgg</code> 删除当前行之前所有行（不包括当前行）</li><li><code>jdG</code>（jd shift + g）   删除当前行之后所有行（不包括当前行）</li><li><code>:1,10d</code> 删除1-10行</li><li><code>:11,$d</code> 删除11行及以后所有的行</li><li><code>:1,$d</code> 删除所有行</li><li><code>J</code>(shift + j)　　删除两行之间的空行，实际上是合并两行。</li></ul><h3 id="拷贝和粘贴"><a href="#拷贝和粘贴" class="headerlink" title="拷贝和粘贴"></a>拷贝和粘贴</h3><ul><li><code>yy</code> 拷贝当前行</li><li><code>nyy</code> 拷贝当前后开始的n行，比如2yy拷贝当前行及其下一行。</li><li><code>p</code>  在当前光标后粘贴,如果之前使用了yy命令来复制一行，那么就在当前行的下一行粘贴。</li><li><code>shift+p</code> 在当前行前粘贴</li><li><code>:1,10 co 20</code> 将1-10行插入到第20行之后。</li><li><code>:1,$ co $</code> 将整个文件复制一份并添加到文件尾部。</li><li>正常模式下按<code>v</code>（逐字）或<code>V</code>（逐行）进入可视模式，然后用jklh命令移动即可选择某些行或字符，再按<code>y</code>即可复制</li><li><code>ddp</code>交换当前行和其下一行</li><li><code>xp</code>交换当前字符和其后一个字符</li></ul><h3 id="剪切命令"><a href="#剪切命令" class="headerlink" title="剪切命令"></a>剪切命令</h3><ul><li>正常模式下按<code>v</code>（逐字）或<code>V</code>（逐行）进入可视模式，然后用jklh命令移动即可选择某些行或字符，再按<code>d</code>即可剪切</li><li><code>ndd</code> 剪切当前行之后的n行。利用p命令可以对剪切的内容进行粘贴</li><li><code>:1,10d</code> 将1-10行剪切。利用p命令可将剪切后的内容进行粘贴。</li><li><code>:1, 10 m 20</code> 将第1-10行移动到第20行之后。</li></ul><h3 id="退出命令"><a href="#退出命令" class="headerlink" title="退出命令"></a>退出命令</h3><ul><li><code>:wq</code> 保存并退出</li><li><code>ZZ</code> 保存并退出</li><li><code>:q!</code> 强制退出并忽略所有更改</li><li><code>:e!</code> 放弃所有修改，并打开原来文件。</li></ul><h3 id="窗口命令"><a href="#窗口命令" class="headerlink" title="窗口命令"></a>窗口命令</h3><ul><li><code>:split</code>或<code>:new</code> 打开一个新窗口，光标停在顶层的窗口上</li><li><code>:split file</code>或<code>:new file</code> 用新窗口打开文件</li><li><code>split</code>打开的窗口都是横向的，使用<code>vsplit</code>可以纵向打开窗口。</li><li><code>Ctrl+ww</code> 移动到下一个窗口</li><li><code>Ctrl+wj</code> 移动到下方的窗口</li><li><code>Ctrl+wk</code> 移动到上方的窗口</li></ul><h3 id="关闭窗口"><a href="#关闭窗口" class="headerlink" title="关闭窗口"></a>关闭窗口</h3><ul><li><code>:close</code> 最后一个窗口不能使用此命令，可以防止意外退出vim。</li><li><code>:q</code> 如果是最后一个被关闭的窗口，那么将退出vim。</li><li><code>ZZ</code> 保存并退出。</li><li>关闭所有窗口，只保留当前窗口<code>:only</code> </li></ul><h3 id="录制宏"><a href="#录制宏" class="headerlink" title="录制宏"></a>录制宏</h3><ul><li>按<code>q</code>键加任意字母开始录制，再按q键结束录制（这意味着vim中的宏不可嵌套），使用的时候@加宏名，比如qa。。。q录制名为a的宏，<code>@a</code>使用这个宏。</li></ul><h3 id="执行shell命令"><a href="#执行shell命令" class="headerlink" title="执行shell命令"></a>执行shell命令</h3><ul><li><code>:!command</code></li><li><code>:!ls 列出当前目录下文件</code></li><li><code>:!perl -c script.pl</code> 检查perl脚本语法，可以不用退出vim，非常方便。</li><li><code>:!perl script.pl</code> 执行perl脚本，可以不用退出vim，非常方便。</li><li><code>:suspend</code>或<code>Ctrl - Z</code> 挂起vim，回到shell，按<code>fg</code>可以返回vim。</li></ul><h3 id="注释命令"><a href="#注释命令" class="headerlink" title="注释命令"></a>注释命令</h3><p>perl程序中#开始的行为注释，所以要注释某些行，只需在行首加入#</p><ul><li><code>3,5 s/^/#/g</code> 注释第3-5行</li><li><code>3,5 s/^#//g</code> 解除3-5行的注释</li><li><code>1,$ s/^/#/g</code> 注释整个文档。</li><li><code>:%s/^/#/g</code> 注释整个文档，此法更快。</li></ul><h3 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h3><ul><li><code>:help</code> or F1 显示整个帮助</li><li><code>:help xxx</code> 显示xxx的帮助，比如 :help i, :help CTRL-[（即Ctrl+[的帮助）。</li><li><code>:help 'number'</code> Vim选项的帮助用单引号括起</li><li><code>:help &lt;Esc&gt;</code> 特殊键的帮助用&lt;&gt;扩起</li><li><code>:help -t</code> Vim启动参数的帮助用-</li><li><code>：help i_&lt;Esc&gt;</code> 插入模式下Esc的帮助，某个模式下的帮助用模式_主题的模式<br>帮助文件中位于||之间的内容是超链接，可以用Ctrl+]进入链接，Ctrl+o（Ctrl + t）返回<br>其他非编辑命令</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> VIM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VIM </tag>
            
            <tag> 基本命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vim-ctrlp-files-search</title>
      <link href="/2019/12/02/vim-ctrlp-files-search/"/>
      <url>/2019/12/02/vim-ctrlp-files-search/</url>
      
        <content type="html"><![CDATA[<p>使用频率最高的插件之一</p><p>作用: 模糊搜索, 可以搜索文件/buffer/mru/tag等等</p><p>github: 原始<a href="https://github.com/kien/ctrlp.vim" target="_blank" rel="noopener">kien/ctrlp</a>, 使用的是国人改进版本 <a href="https://github.com/ctrlpvim/ctrlp.vim" target="_blank" rel="noopener">ctrlpvim/ctrlp.vim</a></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><pre><code>Bundle 'ctrlpvim/ctrlp.vim'</code></pre><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>绑定快捷键</p><p><code>&lt;leader&gt;-f</code>模糊搜索最近打开的文件(MRU)</p><p><code>&lt;leader&gt;-p</code>模糊搜索当前目录及其子目录下的所有文件</p><p>搜索框出来后, 输入关键字, 然后</p><pre><code>ctrl + j/k 进行上下选择ctrl + x 在当前窗口水平分屏打开文件ctrl + v 同上, 垂直分屏ctrl + t 在tab中打开</code></pre><p><img src="http://wklken.me/imgs/vim/ctrlp.gif" alt="ctrlp.gif"></p><h2 id="最终配置"><a href="#最终配置" class="headerlink" title="最终配置"></a>最终配置</h2><pre><code>Bundle 'ctrlpvim/ctrlp.vim'let g:ctrlp_map = '&lt;leader&gt;p'let g:ctrlp_cmd = 'CtrlP'map &lt;leader&gt;f :CtrlPMRU&lt;CR&gt;let g:ctrlp_custom_ignore = {    \ 'dir':  '\v[\/]\.(git|hg|svn|rvm)$',    \ 'file': '\v\.(exe|so|dll|zip|tar|tar.gz|pyc)$',    \ }let g:ctrlp_working_path_mode=0let g:ctrlp_match_window_bottom=1let g:ctrlp_max_height=15let g:ctrlp_match_window_reversed=0let g:ctrlp_mruf_max=500let g:ctrlp_follow_symlinks=1</code></pre><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol><li>更多操作, 详见 <a href="https://github.com/ctrlpvim/ctrlp.vim#once-ctrlp-is-open" target="_blank" rel="noopener">文档</a></li><li>可以考虑废弃<code>fuzzyfinder</code> / <code>Command-T</code></li><li>可以考虑只用一个快捷键, 配置映射到<code>:CtrlPMixed</code>, 就可以一键搜索文件/buffer/mru</li></ol><hr><h2 id="附-ctrlp的插件ctrlp-funky"><a href="#附-ctrlp的插件ctrlp-funky" class="headerlink" title="附: ctrlp的插件ctrlp-funky"></a>附: ctrlp的插件<code>ctrlp-funky</code></h2><p>作用: 模糊搜索当前文件中所有函数</p><p>github: <a href="https://github.com/tacahiroy/ctrlp-funky" target="_blank" rel="noopener">ctrlp-funky</a></p><h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><pre><code>Bundle 'tacahiroy/ctrlp-funky'</code></pre><h2 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h2><p>绑定快捷键</p><p><code>&lt;leader&gt;fu</code> 进入当前文件的函数列表搜索</p><p><code>&lt;leader&gt;fU</code> 搜索当前光标下单词对应的函数</p><p><img src="http://wklken.me/imgs/vim/ctrlp-funky.gif" alt="ctrlp-funky.gif"></p><h2 id="最终配置-1"><a href="#最终配置-1" class="headerlink" title="最终配置"></a>最终配置</h2><pre><code>Bundle 'tacahiroy/ctrlp-funky'nnoremap &lt;Leader&gt;fu :CtrlPFunky&lt;Cr&gt;" narrow the list down with a word under cursornnoremap &lt;Leader&gt;fU :execute 'CtrlPFunky ' . expand('&lt;cword&gt;')&lt;Cr&gt;let g:ctrlp_funky_syntax_highlight = 1let g:ctrlp_extensions = ['funky']</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> VIM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VIM </tag>
            
            <tag> ctrlp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vim-split-window</title>
      <link href="/2019/12/02/vim-split-window/"/>
      <url>/2019/12/02/vim-split-window/</url>
      
        <content type="html"><![CDATA[<h4 id="分屏启动Vim"><a href="#分屏启动Vim" class="headerlink" title="分屏启动Vim"></a>分屏启动Vim</h4><ol><li><p>使用大写的O参数来垂直分屏。</p><pre><code>vim -On file1 file2 ...</code></pre></li><li><p>使用小写的o参数来水平分屏。</p><pre><code>vim -on file1 file2 ...</code></pre></li></ol><p><strong>注释:</strong> n是数字，表示分成几个屏。</p><h4 id="关闭分屏"><a href="#关闭分屏" class="headerlink" title="关闭分屏"></a>关闭分屏</h4><ol><li><p>关闭当前窗口。</p><pre><code>Ctrl+W c</code></pre></li><li><p>关闭当前窗口，如果只剩最后一个了，则退出Vim。</p><pre><code>Ctrl+W q</code></pre></li></ol><h4 id="分屏"><a href="#分屏" class="headerlink" title="分屏"></a>分屏</h4><ol><li><p>上下分割当前打开的文件。</p><pre><code>Ctrl+W s</code></pre></li><li><p>上下分割，并打开一个新的文件。</p><pre><code>:sp filename</code></pre></li><li><p>左右分割当前打开的文件。</p><pre><code>Ctrl+W v</code></pre></li><li><p>左右分割，并打开一个新的文件。</p><pre><code>:vsp filename</code></pre></li></ol><h4 id="移动光标"><a href="#移动光标" class="headerlink" title="移动光标"></a>移动光标</h4><p>Vi中的光标键是h, j, k, l，要在各个屏间切换，只需要先按一下Ctrl+W</p><ol><li><p>把光标移到</p><p>右边</p><p>的屏。</p><pre><code>Ctrl+W l</code></pre></li><li><p>把光标移到</p><p>左边</p><p>的屏中。</p><pre><code>Ctrl+W h</code></pre></li><li><p>把光标移到</p><p>上边</p><p>的屏中。</p><pre><code>Ctrl+W k</code></pre></li><li><p>把光标移到</p><p>下边</p><p>的屏中。</p><pre><code>Ctrl+W j</code></pre></li><li><p>把光标移到</p><p>下一个</p><p>的屏中。.</p><pre><code>Ctrl+W w</code></pre></li></ol><h4 id="移动分屏"><a href="#移动分屏" class="headerlink" title="移动分屏"></a>移动分屏</h4><p>这个功能还是使用了Vim的光标键，只不过都是大写。当然了，如果你的分屏很乱很复杂的话，这个功能可能会出现一些非常奇怪的症状。</p><ol><li><p>向右移动。</p><pre><code>Ctrl+W L</code></pre></li><li><p>向左移动</p><pre><code>Ctrl+W H</code></pre></li><li><p>向上移动</p><pre><code>Ctrl+W K</code></pre></li><li><p>向下移动</p><pre><code>Ctrl+W J</code></pre></li></ol><h4 id="屏幕尺寸"><a href="#屏幕尺寸" class="headerlink" title="屏幕尺寸"></a>屏幕尺寸</h4><p>下面是改变尺寸的一些操作，主要是高度，对于宽度你可以使用[Ctrl+W &lt;]或是[Ctrl+W &gt;]，但这可能需要最新的版本才支持。</p><ol><li><p>让所有的屏都有一样的高度。</p><pre><code>Ctrl+W =</code></pre></li><li><p>增加高度。</p><pre><code>Ctrl+W +</code></pre></li><li><p>减少高度。</p><pre><code>Ctrl+W -</code></pre></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> VIM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VIM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络表示学习论文用到的数据集</title>
      <link href="/2019/11/26/graph-embeddings-experiment-dataset/"/>
      <url>/2019/11/26/graph-embeddings-experiment-dataset/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>将看过的论文中实验部分用到的相关数据集进行了梳理，方便查阅。</p><h2 id="一、HARP"><a href="#一、HARP" class="headerlink" title="一、HARP"></a>一、HARP</h2><blockquote><p>HARP: Hierarchical Representation Learning for Networks </p></blockquote><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul><li>DBLP (Perozzi et al. 2017) –计算机相关科研人员论文合作图。标签表明研究人员发表其工作的研究领域。该数据集包括四个研究领域:DB、DM、IR和ML。29199个顶点，133664条边，4种类别。</li><li>BlogCatalog (Tang and Liu 2009)– BlogCatalog网站上用户之间的社会关系网络。标签代表博客作者发布的类别。</li><li>CiteSeer (Sen et al. 2008)– CiteSeer是计算机科学出版物之间的引用网络。标签标明论文所属的研究领域，论文分为6类:agent, AI, DB, IR, ML, HCI。</li></ul><h3 id="评测实验"><a href="#评测实验" class="headerlink" title="评测实验"></a>评测实验</h3><h4 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h4><p>参考的DeepWalk论文的实验步骤，选择多标签分类作为下游任务，基本步骤如下：</p><ol><li>获得输入图的图表示学习结果</li><li>然后从图中随机抽取一部分节点及其标签作为训练数据，预测剩余节点的标签</li><li>训练一个obe-vs-rest的LR(L2)模型(可以替换其他模型，但这个简单方便)</li><li>重复试验10次，保证结果的合理性。评测指标为average Macro F1、Micro F1和accuracy。</li></ol><p>可以选取不同比例的标签数据(论文中0.02-0.10，每次递增0.01)，重复试验，可以画出增长趋势图，给出更直观的对比结果。</p><h4 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h4><p>从以下几点说明：</p><ol><li>引入HARP不会增加复杂度</li><li>比较前后运行时间，只带来10%的时间开销(在Erdos-Renyi graphs 上进行实验)</li></ol><h2 id="二、-struct2vec"><a href="#二、-struct2vec" class="headerlink" title="二、 struct2vec"></a>二、 struct2vec</h2><blockquote><p>struc2vec: Learning Node Representations from Structural Identity </p></blockquote><h3 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h3><ul><li>Barbell graph：作者自己做的测试图</li><li>Karate network：34个节点，78条边，参考论文Wayne W Zachary. 1977. An information ow model for conflict and fission in small groups. Journal of anthropological research (1977).  其中每个节点表示一个俱乐部成员，边代表成员之间存在友谊关系。</li></ul><h3 id="评测实验-1"><a href="#评测实验-1" class="headerlink" title="评测实验"></a>评测实验</h3><p>分类任务。但Struct2vec更适合用于分类与结构强相关的分类任务。为此，使用的网络是air-traffic networks:无权重、无向图，节点对应于机场，边表示飞机飞行路线。</p><ul><li>Brazilian air-trffic network： 来自国家民用航空局，2016年1月到12月。网络一共有131个节点，1038条边。</li><li>American air-traffic network: 来自运输统计局2016年1月到10月的数据，1190个节点，13599条边。</li><li>European air-traffic network: 来自欧盟统计局2016年1月到12月的数据，399个节点，5995条边。</li></ul><h2 id="三、metapath2vec"><a href="#三、metapath2vec" class="headerlink" title="三、metapath2vec"></a>三、metapath2vec</h2><blockquote><p>metapath2vec: Sclaable representation  learning for heterogeneous network</p></blockquote><h3 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h3><ul><li>Aminer Computer Science dataser: 9323739个计算机科学家和3194405篇论文来自3883个计算机会议(包括会议和期刊)，时间截止到2016年。</li><li>Database and Information System dataset: 464个会议，top-5000作者，72902篇文献。</li></ul><h3 id="评测实验-2"><a href="#评测实验-2" class="headerlink" title="评测实验"></a>评测实验</h3><h4 id="节点分类"><a href="#节点分类" class="headerlink" title="节点分类"></a>节点分类</h4><p>多分类任务，节点表示是从全集上学的。分类器选用逻辑斯蒂回归。训练集从5%到90%采样依次实验，剩下的作为测试集。每次重复十次，求Macro-F1和Micro-F1平均值。对比其他方法DeepWalk\node2vec、LINE(1st+2st)、PTE等，得出两个大表。</p><h5 id="参数敏感性-parameter-sensitivity"><a href="#参数敏感性-parameter-sensitivity" class="headerlink" title="参数敏感性 parameter sensitivity"></a>参数敏感性 parameter sensitivity</h5><p>随机游走条数、随机游走的长度、维度、邻居数量等参数全部取不同值进行实验，绘图。得出4个折线变化图，纵轴代表Macro-F1和Micro-F1值，两条线。</p><h4 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h4><p>利用k-means算法对数据进行聚类，并根据归一化的互信息评估聚类结果。</p><h5 id="参数敏感性"><a href="#参数敏感性" class="headerlink" title="参数敏感性"></a>参数敏感性</h5><p>随机游走条数、随机游走的长度、维度、邻居数量等参数全部取不同值进行实验，绘图。</p><h4 id="Case-Study-Similarity-Search"><a href="#Case-Study-Similarity-Search" class="headerlink" title="Case Study: Similarity Search"></a>Case Study: Similarity Search</h4><p>针对一个节点选取top-10的相似节点，进行绘图，然后看看相似的节点是否在一起。使用2D t-SNE算法。</p><h4 id="Case-Study-Visualization"><a href="#Case-Study-Visualization" class="headerlink" title="Case Study: Visualization"></a>Case Study: Visualization</h4><h4 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h4><p>一般有几种做法，从理论上分析算法时间复杂度。</p><p>本文使用了类似word2vec和node2vec的并行优化策略。试验了不同线程数量并行下的加速情况，并画图，可以看出随着线程数量增长速度线性增长。</p><p>也可以采集节点数量，依次变多，看看运行时间的变化，从小图到大图，来证明scalability。</p><h2 id="四、TGEPS"><a href="#四、TGEPS" class="headerlink" title="四、TGEPS"></a>四、TGEPS</h2><blockquote><p>TGE-PS: Text-driven Graph Embedding with Pairs Sampling</p></blockquote><p>主要做了连接预测和节点分类，在7个数据集上。</p><h3 id="数据集-3"><a href="#数据集-3" class="headerlink" title="数据集"></a>数据集</h3><ul><li>Cora: 2211个节点，5214条边，7种类型</li><li>Facebook: 4039个节点，88234条边</li><li>zhihu: 10000个节点，43894条边</li><li>BlogCatalog: 10312个节点，333983条边， 39种类型</li><li>arVix AstroPh: 18772个节点，1981110条边</li><li>arXiv HepTh:  27400个节点，352542条边，论文摘要做为文本</li><li>Systemized Nomenlature of Medicine - Clinical Terms(SNOMED CT)： 391892个节点，2047749条边， 每个节点的最长描述做为文本</li></ul><h3 id="评测实验-3"><a href="#评测实验-3" class="headerlink" title="评测实验"></a>评测实验</h3><ul><li>连接预测任务，计算AUC值。分两种计算方式：AUC_pair和AUC_LR，使用罗斯蒂回归模型。移除20%的边，并保证每个节点有条边，然后移除的做为测试集。</li><li>节点分类任务，计算macro-F1值。</li></ul><p>两个任务加文本与不加文本做了对比，并和LINE、DeepWalk、Node2Vec做了对比。</p><p>word-embedding和char-embedding加与不加也做了对比。</p><h2 id="五、Paper2vec"><a href="#五、Paper2vec" class="headerlink" title="五、Paper2vec"></a>五、Paper2vec</h2><blockquote><p>Paper2vec: Combining Graph and Text Information for Scientific Paper Representation</p></blockquote><h3 id="数据集-4"><a href="#数据集-4" class="headerlink" title="数据集"></a>数据集</h3><ul><li>CORA ML subset: 来自CORA数据集中关于机器学习论文的子集合。包括7大类2708篇论文，5429个引用连接。文本信息包括标题、摘要和论文引用包括的所有句子。</li><li>CORA full dataset： 51905篇论文、7大类，并手工删除了重复的实体和论文。结果保留36954篇论文和132968个引用连接。</li><li>DELP引用网络(version2)：是一个关于计算机科学相关论文的巨大数据库，只提供引用连接信息和论文标题。为了得到论文全文，借助了其他人的工作。最终得到465355篇论文，但只有224836篇论文有引用连接。通过稳重的手工创建边，最后一共412806个定点，2301292条边。一共24个类别。</li></ul><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>DeepWalk、TADW、LINE、Paragraph Vector、tf-idf、Concatenated baseline</p><h3 id="评测实验-4"><a href="#评测实验-4" class="headerlink" title="评测实验"></a>评测实验</h3><h4 id="节点分类-1"><a href="#节点分类-1" class="headerlink" title="节点分类"></a>节点分类</h4><p>使用SVM分类器。</p><h4 id="连接预测"><a href="#连接预测" class="headerlink" title="连接预测"></a>连接预测</h4><p>使用LR，移除25%的引用连接，并添加随机的。</p><p>对比实验分三大块，一个是只用图网络(LINE、DeepWalk)，一个是只用文本(Paragraph Vector、tf-idf)，另外就是两者都用(Concatenated baseline、TADW、Paper2vec)，分别在三个数据集上实验。</p><h2 id="六、OhmNet"><a href="#六、OhmNet" class="headerlink" title="六、OhmNet"></a>六、OhmNet</h2><h3 id="贡献点"><a href="#贡献点" class="headerlink" title="贡献点"></a>贡献点</h3><p>提出OhmNet，通过对组织层次结构中组织之间的关系进行编码，然后对组织层次结构使用结构化正则化，对组织分类法约束进行建模。 通过这种方式，OhmNet可以有效地学习与组织层次结构一致的蛋白质的多尺度特征表示。其实就是将多个组织看做多个图，学习结点(也就是蛋白质)在多个图的情况下的嵌入式表示。因为同一种在蛋白质在不同的组织中可能起到不同的作用，担任的角色不同。</p><p>针对多层网络，学多个函数$f_1,f_2, \dots, f_K$，每个函数将结点映射到嵌入式空间中。</p><p>OhmNet有两部分组成，第一部分是单层网络目标函数，每层中具有相似邻居的结点在嵌入空间中也临近。第二部分是层级依赖目标函数，相邻层中的结点共享相似的特征。</p><h4 id="单层"><a href="#单层" class="headerlink" title="单层"></a>单层</h4><p>基本思想是相似网络邻居的结点有相似的嵌入表示。</p><p>假设图$G_i$，结点特征$f_i$，给定结点$u \in V_i$，目标函数$w_i$根据学习到的结点特征$f_i$预测哪些结点属于$u$的邻居结点$N_i(u)$之一。</p><p>$$w_i(u) = log(P_r(N_i(u)|f_i(u)))$$，</p><p>其中结点-邻居节点对的条件似然概率可以表示为:</p><p>$$P_r(N_i(u)|f_i(u)) = \prod_{v \in N_i(u)}P_r(v|f_i(u))$$</p><p>条件似然是由节点特征点击之后得到的参数化的softmax单元，它对应于单层前馈神经网络：</p><p>$$P_r(v|f_i(u)) = \frac{\exp(f_i(v)f_i(u))}{\sum_{z \in V_i}\exp(f_i(z)f_i(u))}$$，</p><p>给定结点$u$，最大化$w_i(u)$即最大化分类结点和其邻居，基于其结点表示。最终目标函数为$\Omega_i$</p><p>$$\Omega_i = \sum_{u \in V_i}w_i(u)， for i = 1,2,\dots,K$$</p><h4 id="层级依赖目标函数"><a href="#层级依赖目标函数" class="headerlink" title="层级依赖目标函数"></a>层级依赖目标函数</h4><p>如果仅有单层目标函数，不同网络层中的相同实体结点就会有不同的独立学习出来的嵌入式表示，为了利用层之间的依赖关系，使用属于在层之间共享蛋白质特征。基本假设是层次结构中的相邻层在语义上彼此接近，因此它们中的蛋白质/节点应具有相似的特征。</p><p>假设层级结构为$M$，对于结点$u$，在层级结构$M$中的第$i$个元素使用如下形式的正则化：</p><p>$$c_i(u) = \frac12 ||f_i(u) - f_{\pi(i)}(u) ||_2^2$$</p><p>在欧几里得范式下，这种递归形式的正则化将层级$i$中的节点$u$的特征使得与$i$的父级$\pi(i)$的节点$u$的特征相似。对层级结构中的所有结点都正则化之后，得到</p><p>$$C_i = \sum_{u \in L_i} c_i(u), where L_i = U_{j \in T_i}V_j$$</p><h4 id="整体模型"><a href="#整体模型" class="headerlink" title="整体模型"></a>整体模型</h4><p>给定多层网络$G_1, G_2, \dots, G_k$，整体最大似然优化目标如下所示：</p><p>$$max_{f_1, f_2,\dots,f|M|}\sum_{i \in T}\Omega_i - \lambda\sum_{j \in M}C_j$$</p><p>其中$\lambda$是人为控制的超参数，代表正则强度。上式是非凸的，因为单个目标函数是非凸的，解决 办法是负采样+梯度下降。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol><li>预测细胞功能</li></ol><p>可以看做多标签结点分类任务，每个结点可能有一个或多个标签。OhmNet为每一层中的每个节点以无监督的方式学习了单独的特征向量。因此，对于每层和每个函数，训练一个单独的one0-vs-all的分类器，使用的是带elastic net正则化的Huber loss。并使用了交叉验证，90%数据做为训练集，预测10%的结点标签。</p><p>对比baseline：RESCAL、Minimum curvilinear embedding、LINE、Node2vec，通用</p><p>其他baseline(特定数据集)：GeneMania、Tissue-specific network propagation、Network-based tissue-specific SVM</p><p>两种网络表示：</p><ul><li>独立层：这种方法通过一次在一层上运行LINE或Node2vec算法，并且独立于网络中的其他层，从而了解每一层节点的功能。</li><li>折叠层：此方法首先通过将代表不同层中相同实体的节点相互连接，将层聚合为一个更大的网络。 然后，它学习聚合后网络中节点的嵌入式表示。</li></ul><p>性能指标用AUROC和AUPRC。</p><ol start="2"><li>细胞功能迁移到一个新的组织</li></ol><p>尝试迁移一个或多个层中学到的知识，预测细胞将转移到哪个目标组织层。</p><ol start="3"><li>脑组织的多尺度模型</li></ol><p>首先通过整合九种大脑特异性蛋白质相互作用网络来构建多层大脑网络，每个都是多层网络中的一层，这些层都是按照两层层次结构进行组织的。学习到结点的特征以后，使用2-D降维并可视化。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph Embeddings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>graph-embeddings-papers-list</title>
      <link href="/2019/11/18/graph-embeddings-papers-list/"/>
      <url>/2019/11/18/graph-embeddings-papers-list/</url>
      
        <content type="html"><![CDATA[<h1 id="awesome-network-embedding"><a href="#awesome-network-embedding" class="headerlink" title="awesome-network-embedding"></a>awesome-network-embedding</h1><blockquote><p>原库地址：<a href="https://github.com/chihming/awesome-network-embedding" target="_blank" rel="noopener">https://github.com/chihming/awesome-network-embedding</a></p></blockquote><p><a href="https://github.com/sindresorhus/awesome" target="_blank" rel="noopener"><img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" alt="Awesome"></a><br><a href="http://makeapullrequest.com" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a><br><a href="https://gitter.im/awesome-network-embedding/Lobby" target="_blank" rel="noopener"><img src="https://badges.gitter.im/Join%20Chat.svg" alt="Gitter chat for developers at https://gitter.im/dmlc/xgboost"></a></p><p>Also called network representation learning, graph embedding, knowledge embedding, etc.</p><p>The task is to learn the representations of the vertices from a given network.</p><p>CALL FOR HELP: I’m planning to re-organize the papers with clear classification index in the near future. Please feel free to submit a commit if you find any interesting related work:)</p><img src="https://user-images.githubusercontent.com/8847689/69019860-74d16300-09ed-11ea-968f-5cb696336084.png" width="480"><h2 id="Paper-References-with-the-implementation-s"><a href="#Paper-References-with-the-implementation-s" class="headerlink" title="Paper References with the implementation(s)"></a>Paper References with the implementation(s)</h2><ul><li><strong>GEMSEC</strong><ul><li>GEMSEC: Graph Embedding with Self Clustering, ASONAM 2019</li><li><a href="https://arxiv.org/abs/1802.03997" target="_blank" rel="noopener">[Paper]</a></li><li><a href="https://github.com/benedekrozemberczki/GEMSEC" target="_blank" rel="noopener">[Python]</a> </li></ul></li><li><strong>AmpliGraph</strong><ul><li>Library for learning knowledge graph embeddings with TensorFlow </li><li><a href="http://docs.ampligraph.org" target="_blank" rel="noopener">[Project]</a></li><li><a href="https://github.com/Accenture/AmpliGraph" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>jodie</strong><ul><li>Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks, KDD’19</li><li><a href="http://snap.stanford.edu/jodie/" target="_blank" rel="noopener">[Project]</a></li><li><a href="https://github.com/srijankr/jodie/" target="_blank" rel="noopener">[Code]</a></li></ul></li><li><strong>PyTorch-BigGraph</strong><ul><li>Pytorch-BigGraph - a distributed system for learning graph embeddings for large graphs, SysML’19</li><li><a href="https://github.com/facebookresearch/PyTorch-BigGraph" target="_blank" rel="noopener">[github]</a></li></ul></li><li><strong>ATP</strong><ul><li>ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation, AAAI’19</li><li><a href="https://arxiv.org/abs/1811.00839" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/zhenv5/atp" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>MUSAE</strong><ul><li>Multi-scale Attributed Node Embedding, ArXiv 2019</li><li><a href="https://arxiv.org/abs/1909.13021" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/MUSAE" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>SEAL-CI</strong><ul><li>Semi-Supervised Graph Classification: A Hierarchical Graph Perspective, WWW’19</li><li><a href="https://arxiv.org/pdf/1904.05003.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/SEAL-CI" target="_blank" rel="noopener">[Python PyTorch]</a></li></ul></li><li><strong>N-GCN</strong><ul><li>A Higher-Order Graph Convolutional Layer, NIPS’18 (workshop)</li><li><a href="http://sami.haija.org/papers/high-order-gc-layer.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/MixHop-and-N-GCN" target="_blank" rel="noopener">[Python PyTorch]</a></li></ul></li><li><strong>CapsGNN</strong><ul><li>Capsule Graph Neural Network, ICLR’19</li><li><a href="https://openreview.net/forum?id=Byl8BnRcYm" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/CapsGNN" target="_blank" rel="noopener">[Python PyTorch]</a></li></ul></li><li><strong>Splitter</strong><ul><li>Splitter: Learning Node Representations that Capture Multiple Social Contexts, WWW’19</li><li><a href="http://epasto.org/papers/www2019splitter.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/Splitter" target="_blank" rel="noopener">[Python PyTorch]</a></li></ul></li><li><strong>REGAL</strong><ul><li>REGAL: Representation Learning-based Graph Alignment. International Conference on Information and Knowledge Management, CIKM’18</li><li><a href="https://arxiv.org/pdf/1802.06257.pdf" target="_blank" rel="noopener">[arxiv]</a></li><li><a href="https://dl.acm.org/citation.cfm?id=3271788" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/GemsLab/REGAL" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>PyTorch Geometric</strong><ul><li>Fast Graph Representation Learning With PyTorch Geometric</li><li><a href="https://arxiv.org/pdf/1903.02428.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/rusty1s/pytorch_geometric" target="_blank" rel="noopener">[Python PyTorch]</a></li></ul></li><li><strong>TuckER</strong><ul><li>Tensor Factorization for Knowledge Graph Completion, Arxiv’19</li><li><a href="https://arxiv.org/pdf/1901.09590.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/ibalazevic/TuckER" target="_blank" rel="noopener">[Python PyTorch]</a></li></ul></li><li><strong>HypER</strong><ul><li>Hypernetwork Knowledge Graph Embeddings, Arxiv’18</li><li><a href="https://arxiv.org/pdf/1808.07018.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/ibalazevic/HypER" target="_blank" rel="noopener">[Python PyTorch]</a></li></ul></li><li><strong>GWNN</strong><ul><li>Graph Wavelet Neural Network, ICLR’19</li><li><a href="https://openreview.net/forum?id=H1ewdiR5tQ" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork" target="_blank" rel="noopener">[Python PyTorch]</a></li><li><a href="https://github.com/Eilene/GWNN" target="_blank" rel="noopener">[Python TensorFlow]</a></li></ul></li><li><strong>APPNP</strong><ul><li>Combining Neural Networks with Personalized PageRank for Classification on Graphs, ICLR’19</li><li><a href="https://arxiv.org/abs/1810.05997" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/APPNP" target="_blank" rel="noopener">[Python PyTorch]</a></li><li><a href="https://github.com/klicperajo/ppnp" target="_blank" rel="noopener">[Python TensorFlow]</a></li></ul></li><li><strong>role2vec</strong><ul><li>Learning Role-based Graph Embeddings, IJCAI’18</li><li><a href="https://arxiv.org/pdf/1802.02896.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/role2vec" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>AttentionWalk</strong><ul><li>Watch Your Step: Learning Node Embeddings via Graph Attention, NIPS’18</li><li><a href="https://arxiv.org/pdf/1710.09599.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="http://sami.haija.org/graph/context" target="_blank" rel="noopener">[Python]</a></li><li><a href="https://github.com/benedekrozemberczki/AttentionWalk" target="_blank" rel="noopener">[Python PyTorch]</a></li><li><a href="https://github.com/google-research/google-research/tree/master/graph_embedding/watch_your_step/" target="_blank" rel="noopener">[Python TensorFlow]</a></li></ul></li><li><strong>GAT</strong><ul><li>Graph Attention Networks, ICLR’18</li><li><a href="https://arxiv.org/pdf/1710.10903.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/Diego999/pyGAT" target="_blank" rel="noopener">[Python PyTorch]</a></li><li><a href="https://github.com/PetarV-/GAT" target="_blank" rel="noopener">[Python TensorFlow]</a></li></ul></li><li><strong>SINE</strong><ul><li>SINE: Scalable Incomplete Network Embedding, ICDM’18</li><li><a href="https://github.com/benedekrozemberczki/SINE/blob/master/paper.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/SINE/" target="_blank" rel="noopener">[Python PyTorch]</a></li><li><a href="https://github.com/daokunzhang/SINE" target="_blank" rel="noopener">[C++]</a></li></ul></li><li><strong>SGCN</strong><ul><li>Signed Graph Convolutional Network, ICDM’18</li><li><a href="https://github.com/benedekrozemberczki/SGCN/blob/master/sgcn.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/SGCN" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>TENE</strong><ul><li>Enhanced Network Embedding with Text Information, ICPR’18</li><li><a href="https://github.com/benedekrozemberczki/TENE/blob/master/tene_paper.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/TENE" target="_blank" rel="noopener">[Python]</a> </li></ul></li><li><strong>DANMF</strong><ul><li>Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection, CIKM’18</li><li><a href="https://smartyfh.com/Documents/18DANMF.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/DANMF" target="_blank" rel="noopener">[Python]</a></li><li><a href="https://github.com/smartyfh/DANMF" target="_blank" rel="noopener">[Matlab]</a>  </li></ul></li><li><strong>BANE</strong><ul><li>Binarized Attributed Network Embedding, ICDM’18</li><li><a href="https://www.researchgate.net/publication/328688614_Binarized_Attributed_Network_Embedding" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/BANE" target="_blank" rel="noopener">[Python]</a></li><li><a href="https://github.com/ICDM2018-BANE/BANE" target="_blank" rel="noopener">[Matlab]</a></li></ul></li><li><strong>GCN Insights</strong><ul><li>Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, AAAI’18</li><li><a href="https://liqimai.github.io/blog/AAAI-18/" target="_blank" rel="noopener">[Project]</a></li><li><a href="https://github.com/liqimai/gcn/tree/AAAI-18/" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>PCTADW</strong><ul><li>Learning Embeddings of Directed Networks with Text-Associated Nodes—with Applications in Software Package Dependency Networks</li><li><a href="https://arxiv.org/pdf/1809.02270.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/shudan/PCTADW" target="_blank" rel="noopener">[Python]</a></li><li><a href="https://doi.org/10.5281/zenodo.1410669" target="_blank" rel="noopener">[dataset]</a></li></ul></li><li><strong>LGCN</strong><ul><li>Large-Scale Learnable Graph Convolutional Networks, KDD’18</li><li><a href="http://www.kdd.org/kdd2018/accepted-papers/view/large-scale-learnable-graph-convolutional-networks" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/HongyangGao/LGCN" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>AspEm</strong><ul><li>AspEm: Embedding Learning by Aspects in Heterogeneous Information Networks</li><li><a href="http://yushi2.web.engr.illinois.edu/sdm18.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/ysyushi/aspem" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>Walklets</strong><ul><li>Don’t Walk, Skip! Online Learning of Multi-scale Network Embeddings</li><li><a href="https://arxiv.org/pdf/1605.02115.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/benedekrozemberczki/walklets" target="_blank" rel="noopener">[Python]</a>  </li></ul></li><li><strong>gat2vec</strong><ul><li>gat2vec: Representation learning for attributed graphs</li><li><a href="https://doi.org/10.1007/s00607-018-0622-9" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/snash4/GAT2VEC" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>FSCNMF</strong><ul><li>FSCNMF: Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks</li><li><a href="https://arxiv.org/abs/1804.05313" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/sambaranban/FSCNMF" target="_blank" rel="noopener">[Python]</a>  </li><li><a href="https://github.com/benedekrozemberczki/FSCNMF" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>SIDE</strong><ul><li>SIDE: Representation Learning in Signed Directed Networks</li><li><a href="https://datalab.snu.ac.kr/side/resources/side.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://datalab.snu.ac.kr/side/resources/side.zip" target="_blank" rel="noopener">[Python]</a></li><li><a href="https://datalab.snu.ac.kr/side/" target="_blank" rel="noopener">[Site]</a></li></ul></li><li><strong>AWE</strong><ul><li>Anonymous Walk Embeddings, ICML’18</li><li><a href="https://www.researchgate.net/publication/325114285_Anonymous_Walk_Embeddings" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/nd7141/Anonymous-Walk-Embeddings" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>BiNE</strong><ul><li>BiNE: Bipartite Network Embedding, SIGIR’18</li><li><a href="http://staff.ustc.edu.cn/~hexn/papers/sigir18-bipartiteNE.pdf" target="_blank" rel="noopener">[paper]</a></li><li><a href="https://github.com/clhchtcjj/BiNE" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>HOPE</strong><ul><li>Asymmetric Transitivity Preserving Graph Embedding</li><li><a href="http://www.kdd.org/kdd2016/papers/files/rfp0184-ouA.pdf" target="_blank" rel="noopener">[KDD 2016]</a></li><li><a href="https://github.com/AnryYang/HOPE" target="_blank" rel="noopener">[Python]</a> </li></ul></li><li><strong>VERSE</strong><ul><li>VERSE, Versatile Graph Embeddings from Similarity Measures</li><li><a href="https://arxiv.org/abs/1803.04742" target="_blank" rel="noopener">[Arxiv]</a> [[WWW 2018]]</li><li><a href="https://github.com/xgfs/verse" target="_blank" rel="noopener">[Python]</a> </li></ul></li><li><strong>AGNN</strong><ul><li>Attention-based Graph Neural Network for semi-supervised learning</li><li><a href="https://openreview.net/forum?id=rJg4YGWRb" target="_blank" rel="noopener">[ICLR 2018 OpenReview (rejected)]</a></li><li><a href="https://github.com/dawnranger/pytorch-AGNN" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>SEANO</strong><ul><li>Semi-supervised Embedding in Attributed Networks with Outliers</li><li><a href="https://arxiv.org/pdf/1703.08100.pdf" target="_blank" rel="noopener">[Paper]</a> (SDM 2018)</li><li><a href="http://jiongqianliang.com/SEANO/" target="_blank" rel="noopener">[Python]</a>   </li></ul></li><li><strong>Hyperbolics</strong><ul><li>Representation Tradeoffs for Hyperbolic Embeddings </li><li><a href="https://arxiv.org/abs/1804.03329" target="_blank" rel="noopener">[Arxiv]</a></li><li><a href="https://github.com/HazyResearch/hyperbolics" target="_blank" rel="noopener">[Python]</a>   </li></ul></li><li><strong>DGCNN</strong><ul><li>An End-to-End Deep Learning Architecture for Graph Classiﬁcation </li><li><a href="http://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf" target="_blank" rel="noopener">[AAAI 2018]</a></li><li><a href="https://github.com/muhanzhang/DGCNN" target="_blank" rel="noopener">[Lua]</a> <a href="https://github.com/muhanzhang/pytorch_DGCNN" target="_blank" rel="noopener">[Python]</a>  </li></ul></li><li><strong>structure2vec</strong><ul><li>Discriminative Embeddings of Latent Variable Models for Structured Data </li><li><a href="https://arxiv.org/abs/1603.05629" target="_blank" rel="noopener">[Arxiv]</a></li><li><a href="https://github.com/Hanjun-Dai/pytorch_structure2vec" target="_blank" rel="noopener">[Python]</a>  </li></ul></li><li><strong>Decagon</strong><ul><li>Decagon, Graph Neural Network for Multirelational Link Prediction </li><li><a href="https://arxiv.org/abs/1802.00543" target="_blank" rel="noopener">[Arxiv]</a> <a href="http://snap.stanford.edu/decagon/" target="_blank" rel="noopener">[SNAP]</a> [[ISMB 2018]]</li><li><a href="https://github.com/marinkaz/decagon" target="_blank" rel="noopener">[Python]</a>    </li></ul></li><li><strong>DHNE</strong><ul><li>Structural Deep Embedding for Hyper-Networks</li><li><a href="http://nrl.thumedialab.com/Structural-Deep-Embedding-for-Hyper-Networks" target="_blank" rel="noopener">[AAAI 2018]</a><a href="https://arxiv.org/abs/1711.10146" target="_blank" rel="noopener">[Arxiv]</a></li><li><a href="https://github.com/tadpole/DHNE" target="_blank" rel="noopener">[Python]</a>  </li></ul></li><li><strong>Ohmnet</strong><ul><li>Feature Learning in Multi-Layer Networks </li><li><a href="https://arxiv.org/abs/1707.04638" target="_blank" rel="noopener">[Arxiv]</a> <a href="http://snap.stanford.edu/ohmnet/" target="_blank" rel="noopener">[SNAP]</a> </li><li><a href="https://github.com/marinkaz/ohmnet" target="_blank" rel="noopener">[Python]</a>  </li></ul></li><li><strong>SDNE</strong><ul><li>Structural Deep Network Embedding </li><li><a href="http://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf" target="_blank" rel="noopener">[KDD 2016]</a></li><li><a href="https://github.com/xiaohan2012/sdne-keras" target="_blank" rel="noopener">[Python]</a> </li></ul></li><li><strong>STWalk</strong><ul><li>STWalk: Learning Trajectory Representations in Temporal Graphs] </li><li><a href="https://arxiv.org/abs/1711.04150" target="_blank" rel="noopener">[Arxiv]</a></li><li><a href="https://github.com/supriya-pandhre/STWalk" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>LoNGAE</strong><ul><li>Learning to Make Predictions on Graphs with Autoencoders </li><li><a href="https://arxiv.org/abs/1802.08352" target="_blank" rel="noopener">[Arxiv]</a></li><li><a href="https://github.com/vuptran/graph-representation-learning" target="_blank" rel="noopener">[Python]</a>  </li></ul></li><li><strong>RSDNE</strong><ul><li><a href="https://zhengwang100.github.io/AAAI18_RSDNE.pdf" target="_blank" rel="noopener">RSDNE: Exploring Relaxed Similarity and Dissimilarity from Completely-imbalanced Labels for Network Embedding.</a>, AAAI 2018</li><li><a href="https://github.com/zhengwang100/RSDNE" target="_blank" rel="noopener">[Matlab]</a> </li></ul></li><li><strong>FastGCN</strong><ul><li>FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling </li><li><a href="https://arxiv.org/abs/1801.10247" target="_blank" rel="noopener">[Arxiv]</a>, <a href="https://openreview.net/forum?id=rytstxWAW" target="_blank" rel="noopener">[ICLR 2018 OpenReview]</a></li><li><a href="https://github.com/matenure/FastGCN" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>diff2vec</strong><ul><li><a href="http://homepages.inf.ed.ac.uk/s1668259/papers/sequence.pdf" target="_blank" rel="noopener">Fast Sequence Based Embedding with Diffusion Graphs</a>, CompleNet 2018</li><li><a href="https://github.com/benedekrozemberczki/diff2vec" target="_blank" rel="noopener">[Python]</a> </li></ul></li><li><strong>Poincare</strong><ul><li><a href="https://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations" target="_blank" rel="noopener">Poincaré Embeddings for Learning Hierarchical Representations</a>, NIPS 2017</li><li><a href="https://github.com/facebookresearch/poincare-embeddings" target="_blank" rel="noopener">[PyTorch]</a> <a href="https://radimrehurek.com/gensim/models/poincare.html" target="_blank" rel="noopener">[Python]</a> <a href="https://github.com/TatsuyaShirakawa/poincare-embedding" target="_blank" rel="noopener">[C++]</a></li></ul></li><li><strong>PEUNE</strong><ul><li><a href="https://papers.nips.cc/paper/7110-prune-preserving-proximity-and-global-ranking-for-network-embedding" target="_blank" rel="noopener">PRUNE: Preserving Proximity and Global Ranking for Network Embedding</a>, NIPS 2017</li><li><a href="https://github.com/ntumslab/PRUNE" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>ASNE</strong><ul><li>Attributed Social Network Embedding, arxiv’17</li><li><a href="https://arxiv.org/abs/1706.01860" target="_blank" rel="noopener">[arxiv]</a></li><li><a href="https://github.com/lizi-git/ASNE" target="_blank" rel="noopener">[Python]</a></li><li><a href="https://github.com/benedekrozemberczki/ASNE" target="_blank" rel="noopener">[Fast Python]</a></li></ul></li><li><strong>GraphWave</strong><ul><li><a href="http://snap.stanford.edu/graphwave/" target="_blank" rel="noopener">Spectral Graph Wavelets for Structural Role Similarity in Networks</a>, </li><li><a href="https://arxiv.org/abs/1710.10321" target="_blank" rel="noopener">[arxiv]</a>, <a href="https://openreview.net/forum?id=rytstxWAW" target="_blank" rel="noopener">[ICLR 2018 OpenReview]</a></li><li><a href="https://github.com/snap-stanford/graphwave" target="_blank" rel="noopener">[Python]</a> <a href="https://github.com/benedekrozemberczki/GraphWaveMachine" target="_blank" rel="noopener">[faster version]</a></li></ul></li><li><strong>StarSpace</strong><ul><li><a href="https://arxiv.org/pdf/1709.03856" target="_blank" rel="noopener">StarSpace: Embed All The Things!</a>, arxiv’17</li><li><a href="https://github.com/facebookresearch/Starspace" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>proNet-core</strong><ul><li>Vertex-Context Sampling for Weighted Network Embedding, arxiv’17</li><li><a href="https://arxiv.org/abs/1711.00227" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/cnclabs/proNet-core" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>struc2vec</strong><ul><li><a href="https://dl.acm.org/citation.cfm?id=3098061" target="_blank" rel="noopener">struc2vec: Learning Node Representations from Structural Identity</a>, KDD’17</li><li><a href="https://github.com/leoribeiro/struc2vec" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>ComE</strong><ul><li>Learning Community Embedding with Community Detection and Node Embedding on Graphs, CIKM’17</li><li><a href="https://github.com/andompesta/ComE" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>BoostedNE</strong><ul><li><a href="https://arxiv.org/abs/1808.08627" target="_blank" rel="noopener">Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation</a>, ‘18</li><li><a href="https://github.com/benedekrozemberczki/BoostedFactorization" target="_blank" rel="noopener">[Python]</a>  </li></ul></li><li><strong>M-NMF</strong><ul><li>Community Preserving Network Embedding, AAAI’17</li><li><a href="https://github.com/benedekrozemberczki/M-NMF" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>GraphSAGE</strong><ul><li>Inductive Representation Learning on Large Graphs, NIPS’17</li><li><a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/williamleif/GraphSAGE" target="_blank" rel="noopener">[TF]</a> <a href="https://github.com/williamleif/graphsage-simple/" target="_blank" rel="noopener">[PyTorch]</a> </li></ul></li><li><strong>ICE</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=3080807" target="_blank" rel="noopener">ICE: Item Concept Embedding via Textual Information</a>, SIGIR’17</li><li><a href="https://cnclabs.github.io/ICE/" target="_blank" rel="noopener">[demo]</a> <a href="https://github.com/cnclabs/ICE" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>GuidedHeteEmbedding</strong><ul><li>Task-guided and path-augmented heterogeneous network embedding for author identification, WSDM’17</li><li><a href="https://arxiv.org/pdf/1612.02814.pdf" target="_blank" rel="noopener">[paper]</a> <a href="https://github.com/chentingpc/GuidedHeteEmbedding" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>metapath2vec</strong><ul><li>metapath2vec: Scalable Representation Learning for Heterogeneous Networks, KDD’17</li><li><a href="https://www3.nd.edu/~dial/publications/dong2017metapath2vec.pdf" target="_blank" rel="noopener">[paper]</a> <a href="https://ericdongyx.github.io/metapath2vec/m2v.html" target="_blank" rel="noopener">[project website]</a></li></ul></li><li><strong>GCN</strong><ul><li>Semi-Supervised Classification with Graph Convolutional Networks, ICLR’17</li><li><a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">[arxiv]</a>  <a href="https://github.com/tkipf/gcn" target="_blank" rel="noopener">[Python Tensorflow]</a></li></ul></li><li><strong>GAE</strong><ul><li>Variational Graph Auto-Encoders, arxiv</li><li><a href="https://arxiv.org/abs/1611.07308" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/tkipf/gae" target="_blank" rel="noopener">[Python Tensorflow]</a></li></ul></li><li><strong>CANE</strong><ul><li>CANE: Context-Aware Network Embedding for Relation Modeling, ACL’17</li><li><a href="http://www.thunlp.org/~tcc/publications/acl2017_cane.pdf" target="_blank" rel="noopener">[paper]</a> <a href="https://github.com/thunlp/cane" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>TransNet</strong><ul><li>TransNet: Translation-Based Network Representation Learning for Social Relation Extraction, IJCAI’17</li><li><a href="https://github.com/thunlp/TransNet" target="_blank" rel="noopener">[Python Tensorflow]</a></li></ul></li><li><strong>cnn_graph</strong><ul><li>Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, NIPS’16</li><li><a href="https://github.com/mdeff/cnn_graph" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>ConvE</strong><ul><li><a href="https://arxiv.org/pdf/1707.01476v2.pdf" target="_blank" rel="noopener">Convolutional 2D Knowledge Graph Embeddings</a>, arxiv</li><li><a href="https://github.com/TimDettmers/ConvE" target="_blank" rel="noopener">[source]</a></li></ul></li><li><strong>node2vec</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=2939672.2939754" target="_blank" rel="noopener">node2vec: Scalable Feature Learning for Networks</a>, KDD’16</li><li><a href="https://arxiv.org/abs/1607.00653" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/aditya-grover/node2vec" target="_blank" rel="noopener">[Python]</a> <a href="https://github.com/apple2373/node2vec" target="_blank" rel="noopener">[Python-2]</a> <a href="https://github.com/eliorc/node2vec" target="_blank" rel="noopener">[Python-3]</a> <a href="https://github.com/xgfs/node2vec-c" target="_blank" rel="noopener">[C++]</a>  </li></ul></li><li><strong>DNGR</strong><ul><li><a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12423" target="_blank" rel="noopener">Deep Neural Networks for Learning Graph Representations</a>, AAAI’16</li><li><a href="https://github.com/ShelsonCao/DNGR" target="_blank" rel="noopener">[Matlab]</a> <a href="https://github.com/MdAsifKhan/DNGR-Keras" target="_blank" rel="noopener">[Python Keras]</a></li></ul></li><li><strong>HolE</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=3016172" target="_blank" rel="noopener">Holographic Embeddings of Knowledge Graphs</a>, AAAI’16</li><li><a href="https://github.com/mnick/holographic-embeddings" target="_blank" rel="noopener">[Python-sklearn]</a> <a href="https://github.com/mnick/scikit-kge" target="_blank" rel="noopener">[Python-sklearn2]</a></li></ul></li><li><strong>ComplEx</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=3045609" target="_blank" rel="noopener">Complex Embeddings for Simple Link Prediction</a>, ICML’16</li><li><a href="https://arxiv.org/abs/1606.06357" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/ttrouill/complex" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>MMDW</strong><ul><li>Max-Margin DeepWalk: Discriminative Learning of Network Representation, IJCAI’16</li><li><a href="http://nlp.csai.tsinghua.edu.cn/~lzy/publications/ijcai2016_mmdw.pdf" target="_blank" rel="noopener">[paper]</a>  <a href="https://github.com/thunlp/MMDW" target="_blank" rel="noopener">[Java]</a></li></ul></li><li><strong>planetoid</strong><ul><li>Revisiting Semi-supervised Learning with Graph Embeddings, ICML’16</li><li><a href="https://arxiv.org/abs/1603.08861" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/kimiyoung/planetoid" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>graph2vec</strong><ul><li>graph2vec: Learning Distributed Representations of Graphs, KDD’17 MLGWorkshop</li><li><a href="https://arxiv.org/abs/1707.05005" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/benedekrozemberczki/graph2vec" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>PowerWalk</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=2983713" target="_blank" rel="noopener">PowerWalk: Scalable Personalized PageRank via Random Walks with Vertex-Centric Decomposition</a>, CIKM’16</li><li><a href="https://github.com/lqhl/PowerWalk" target="_blank" rel="noopener">[code]</a></li></ul></li><li><strong>LINE</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=2741093" target="_blank" rel="noopener">LINE: Large-scale information network embedding</a>, WWW’15</li><li><a href="https://arxiv.org/abs/1503.03578" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/tangjianpku/LINE" target="_blank" rel="noopener">[C++]</a> <a href="https://github.com/snowkylin/line" target="_blank" rel="noopener">[Python TF]</a> <a href="https://github.com/VahidooX/LINE" target="_blank" rel="noopener">[Python Theano/Keras]</a></li></ul></li><li><strong>PTE</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=2783307" target="_blank" rel="noopener">PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks</a>, KDD’15</li><li><a href="https://github.com/mnqu/PTE" target="_blank" rel="noopener">[C++]</a></li></ul></li><li><strong>GraRep</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=2806512" target="_blank" rel="noopener">Grarep: Learning graph representations with global structural information</a>, CIKM’15</li><li><a href="https://github.com/ShelsonCao/GraRep" target="_blank" rel="noopener">[Matlab]</a></li><li><a href="https://github.com/xgfs/GraRep.jl" target="_blank" rel="noopener">[Julia]</a></li><li><a href="https://github.com/benedekrozemberczki/GraRep" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>KB2E</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=2886624" target="_blank" rel="noopener">Learning Entity and Relation Embeddings for Knowledge Graph Completion</a>, AAAI’15</li><li><a href="http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2015_transr.pdf" target="_blank" rel="noopener">[paper]</a> <a href="https://github.com/thunlp/KB2E" target="_blank" rel="noopener">[C++]</a>  <a href="https://github.com/thunlp/Fast-TransX" target="_blank" rel="noopener">[faster version]</a></li></ul></li><li><strong>TADW</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=2832542" target="_blank" rel="noopener">Network Representation Learning with Rich Text Information</a>, IJCAI’15</li><li><a href="https://www.ijcai.org/Proceedings/15/Papers/299.pdf" target="_blank" rel="noopener">[paper]</a> <a href="https://github.com/thunlp/tadw" target="_blank" rel="noopener">[Matlab]</a> <a href="https://github.com/benedekrozemberczki/TADW" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>DeepWalk</strong><ul><li><a href="http://dl.acm.org/citation.cfm?id=2623732" target="_blank" rel="noopener">DeepWalk: Online Learning of Social Representations</a>, KDD’14</li><li><a href="https://arxiv.org/abs/1403.6652" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/phanein/deepwalk" target="_blank" rel="noopener">[Python]</a>  <a href="https://github.com/xgfs/deepwalk-c" target="_blank" rel="noopener">[C++]</a></li></ul></li><li><strong>GEM</strong><ul><li>Graph Embedding Techniques, Applications, and Performance: A Survey</li><li><a href="https://arxiv.org/abs/1705.02801" target="_blank" rel="noopener">[arxiv]</a> <a href="https://github.com/palash1992/GEM" target="_blank" rel="noopener">[Python]</a></li></ul></li><li><strong>DNE-SBP</strong><ul><li>Deep Network Embedding for Graph Representation Learning in Signed Networks</li><li><a href="https://ieeexplore.ieee.org/document/8486671" target="_blank" rel="noopener">[paper]</a> <a href="https://github.com/shenxiaocam/Deep-network-embedding-for-graph-representation-learning-in-signed-networks" target="_blank" rel="noopener">[Code]</a></li></ul></li></ul><h2 id="Paper-References"><a href="#Paper-References" class="headerlink" title="Paper References"></a>Paper References</h2><p><a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener">A Comprehensive Survey on Graph Neural Networks</a>, arxiv’19</p><p><a href="https://arxiv.org/pdf/1806.08804.pdf" target="_blank" rel="noopener">Hierarchical Graph Representation Learning with Differentiable Pooling</a>, NIPS’18</p><p><strong>SEMAC</strong>, <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16442" target="_blank" rel="noopener">Link Prediction via Subgraph Embedding-Based Convex Matrix Completion</a>, AAAI 2018, <a href="https://www.slideshare.net/gdm3003/semac-graph-node-embeddings-for-link-prediction" target="_blank" rel="noopener">Slides</a></p><p><strong>MILE</strong>, <a href="https://arxiv.org/pdf/1802.09612.pdf" target="_blank" rel="noopener">MILE: A Multi-Level Framework for Scalable Graph Embedding</a>, arxiv’18</p><p><strong>MetaGraph2Vec</strong>, <a href="https://arxiv.org/abs/1803.02533" target="_blank" rel="noopener">MetaGraph2Vec: Complex Semantic Path Augmented Heterogeneous Network Embedding</a></p><p><strong>PinSAGE</strong>, <a href="https://arxiv.org/abs/1806.01973" target="_blank" rel="noopener">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</a></p><p><a href="https://dl.acm.org/citation.cfm?id=3159711" target="_blank" rel="noopener">Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning</a>, WSDM ‘18</p><p><a href="https://arxiv.org/abs/1711.07838" target="_blank" rel="noopener">Adversarial Network Embedding</a>, arxiv</p><p><strong>Role2Vec</strong>, <a href="https://arxiv.org/abs/1802.02896" target="_blank" rel="noopener">Learning Role-based Graph Embeddings</a></p><p><strong>edge2vec</strong>, <a href="https://arxiv.org/abs/1804.06111" target="_blank" rel="noopener">Feature Propagation on Graph: A New Perspective to Graph Representation<br>Learning</a></p><p><strong>MINES</strong>, <a href="http://cse.msu.edu/~mayao4/downloads/Multidimensional_Network_Embedding_with_Hierarchical_Structure.pdf" target="_blank" rel="noopener">Multi-Dimensional Network Embedding with Hierarchical Structure</a></p><p><a href="https://arxiv.org/abs/1804.05837" target="_blank" rel="noopener">Walk-Steered Convolution for Graph Classification</a></p><p><a href="https://arxiv.org/abs/1704.08829" target="_blank" rel="noopener">Deep Feature Learning for Graphs</a>, arxiv’17</p><p><a href="https://arxiv.org/abs/1710.10881" target="_blank" rel="noopener">Fast Linear Model for Knowledge Graph Embeddings</a>, arxiv’17</p><p><a href="https://arxiv.org/abs/1710.02971" target="_blank" rel="noopener">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</a>, arxiv’17</p><p><a href="https://arxiv.org/abs/1709.07604" target="_blank" rel="noopener">A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications</a>, arxiv’17</p><p><a href="https://arxiv.org/pdf/1709.05584.pdf" target="_blank" rel="noopener">Representation Learning on Graphs: Methods and Applications</a>, IEEE DEB’17</p><p><strong>CONE</strong>, <a href="https://arxiv.org/abs/1709.01554" target="_blank" rel="noopener">CONE: Community Oriented Network Embedding</a>, arxiv’17</p><p><strong>LANE</strong>,<br><a href="http://dl.acm.org/citation.cfm?id=3018667" target="_blank" rel="noopener">Label Informed Attributed Network Embedding</a>, WSDM’17</p><p><strong>Graph2Gauss</strong>,<br><a href="https://arxiv.org/abs/1707.03815" target="_blank" rel="noopener">Deep Gaussian Embedding of Attributed Graphs: Unsupervised Inductive Learning via Ranking</a>, arxiv<br><a href="https://twitter.com/abojchevski/status/885502050133585925" target="_blank" rel="noopener">[Bonus Animation]</a></p><p><a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14696" target="_blank" rel="noopener">Scalable Graph Embedding for Asymmetric Proximity</a>, AAAI’17</p><p><a href="http://dl.acm.org/citation.cfm?id=2959169" target="_blank" rel="noopener">Query-based Music Recommendations via Preference Embedding</a>, RecSys’16</p><p><a href="http://dl.acm.org/citation.cfm?id=3060886" target="_blank" rel="noopener">Tri-party deep network representation</a>, IJCAI’16</p><p><a href="http://dl.acm.org/citation.cfm?id=2783296" target="_blank" rel="noopener">Heterogeneous Network Embedding via Deep Architectures</a>, KDD’15</p><p><a href="http://dl.acm.org/citation.cfm?id=2969070" target="_blank" rel="noopener">Neural Word Embedding As Implicit Matrix Factorization</a>, NIPS’14</p><p><a href="http://dl.acm.org/citation.cfm?id=2488393" target="_blank" rel="noopener">Distributed large-scale natural graph factorization</a>, WWW’13</p><p><a href="https://arxiv.org/abs/1610.09950" target="_blank" rel="noopener">From Node Embedding To Community Embedding</a>, arxiv</p><p><a href="https://arxiv.org/abs/1605.02115" target="_blank" rel="noopener">Walklets: Multiscale Graph Embeddings for Interpretable Network Classification</a>, arxiv</p><p><a href="https://arxiv.org/abs/1501.00358" target="_blank" rel="noopener">Comprehend DeepWalk as Matrix Factorization</a>, arxiv</p><h1 id="Conference-amp-Workshop"><a href="#Conference-amp-Workshop" class="headerlink" title="Conference &amp; Workshop"></a>Conference &amp; Workshop</h1><p><a href="http://www.mlgworkshop.org/2017/" target="_blank" rel="noopener">13th International Workshop on Mining and Learning with Graphs</a>, <strong>MLG’17</strong></p><p><a href="http://snap.stanford.edu/proj/embeddings-www/" target="_blank" rel="noopener">WWW-18 Tutorial Representation Learning on Networks</a>, <strong>WWW’18</strong></p><h2 id="Related-List"><a href="#Related-List" class="headerlink" title="Related List"></a>Related List</h2><p><a href="https://github.com/benedekrozemberczki/awesome-graph-classification" target="_blank" rel="noopener">awesome-graph-classification</a></p><p><a href="https://github.com/benedekrozemberczki/awesome-community-detection" target="_blank" rel="noopener">awesome-community-detection</a></p><p><a href="https://github.com/Hironsan/awesome-embedding-models" target="_blank" rel="noopener">awesome-embedding-models</a></p><p><a href="https://github.com/thunlp/NRLPapers" target="_blank" rel="noopener">Must-read papers on network representation learning (NRL) / network embedding (NE)</a></p><p><a href="https://github.com/thunlp/KRLPapers" target="_blank" rel="noopener">Must-read papers on knowledge representation learning (KRL) / knowledge embedding (KE)</a></p><p><a href="https://github.com/nate-russell/Network-Embedding-Resources" target="_blank" rel="noopener">Network Embedding Resources</a></p><p><a href="https://github.com/Hironsan/awesome-embedding-models" target="_blank" rel="noopener">awesome-embedding-models</a></p><p><a href="https://github.com/MaxwellRebo/awesome-2vec" target="_blank" rel="noopener">2vec-type embedding models</a></p><h2 id="Related-Project"><a href="#Related-Project" class="headerlink" title="Related Project"></a>Related Project</h2><p><strong>Stanford Network Analysis Project</strong> <a href="http://snap.stanford.edu/" target="_blank" rel="noopener">website</a></p><p><strong>StellarGraph Machine Learning Library</strong> <a href="https://www.stellargraph.io" target="_blank" rel="noopener">website</a> <a href="https://github.com/stellargraph/stellargraph" target="_blank" rel="noopener">GitHub</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Graph Embeddings </category>
          
      </categories>
      
      
        <tags>
            
            <tag> papers </tag>
            
            <tag> graph embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git开发相关操作记录</title>
      <link href="/2019/11/16/git-notes/"/>
      <url>/2019/11/16/git-notes/</url>
      
        <content type="html"><![CDATA[<h2 id="如何对开源项目贡献代码"><a href="#如何对开源项目贡献代码" class="headerlink" title="如何对开源项目贡献代码"></a>如何对开源项目贡献代码</h2><ol><li>Fork开源项目</li><li>将开源项目下载到自己的电脑(git clone xxx)</li><li>创建自己的分支(git checkout -b my-new-branch)</li><li>修改后添加修改(git add .)</li><li>提交修改，并写好描述(git commit -m ‘xxx’)</li><li>提交修改到分支(git push origin my-new-branch)</li><li>发起新的reqeust</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JDK自带工具详解汇总</title>
      <link href="/2019/11/14/java-jvm-4/"/>
      <url>/2019/11/14/java-jvm-4/</url>
      
        <content type="html"><![CDATA[<h1 id="深入理解Java虚拟机读书笔记-3-JDK自带工具详解汇总（不断更新）"><a href="#深入理解Java虚拟机读书笔记-3-JDK自带工具详解汇总（不断更新）" class="headerlink" title="深入理解Java虚拟机读书笔记(3): JDK自带工具详解汇总（不断更新）"></a>深入理解Java虚拟机读书笔记(3): JDK自带工具详解汇总（不断更新）</h1><p>对于初学者来说，学习Java时可能仅仅认识了<code>java</code>和<code>javac</code>两个命令，其他的命令没有基础过。其实jdk自带了非常多的非常好用的工具，在windows环境下，打开jdk下的bin目录，可以看到很多exe文件，如下图所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546841202292.png" alt="1546841202292"></p><p>同样的，在linux环境下jdk的bin目录下也一样提供了很多工具，这些是编译好的二进制可执行脚本：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546841337590.png" alt="1546841337590"></p><p>下面就比较常用的一些工具做具体介绍。</p><blockquote><p>做介绍之前，提供一个官方的文档，所有命令的用法介绍、参数详解、结果详解都可以查询到</p><p>地址为：<a href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/index.html" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/technotes/tools/unix/index.html</a></p><p>此为JDK1.8的，其他版本自己对应查找连接即可。</p><p>下面介绍的时候也是按照官方文档的分类方法。</p></blockquote><h2 id="监控JVM的命令"><a href="#监控JVM的命令" class="headerlink" title="监控JVM的命令"></a>监控JVM的命令</h2><h3 id="一、jps"><a href="#一、jps" class="headerlink" title="一、jps"></a>一、jps</h3><p>对于搞大数据的人来说，jps命令可以说是最常用的命令之一了，它可以用来查看当前运行的虚拟机进程。<a href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jps.html#CHDCGECD" target="_blank" rel="noopener">官方文档地址</a></p><p>使用方法为<code>jps [option] [hostid]</code></p><p>参数介绍：</p><p>-q  仅输出VM标识符，不包括class name,jar name,arguments in main method，也就是仅输出进程ID了<br>-m 输出main method的参数<br>-l   输出完全的包名，应用主类名，jar的完全路径名<br>-v  输出jvm参数<br>-V 输出通过flag文件传递到JVM中的参数.hotspotrc文件或-XX:Flags=所指定的文件<br>-Joption 传递参数到vm,例如:-J-Xms48m</p><p>hostid一般是本地java进行id，也可以是远程的，格式如下</p><p><code>[protocol:][[//]hostname][:port][/servername]</code></p><p><strong>如果需要查看其他机器上的jvm进程，需要在待查看机器上启动jstatd。</strong></p><p> 命令的输出格式 ：<br>lvmid [ [ classname| JARfilename | “Unknown”][ arg* ] [ jvmarg* ] ]</p><p>几种使用情况介绍：</p><p>1） 无任何参数，直接jps，此时默认输出所有jvm进程，并打印ID和主类名</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546842679601.png" alt="1546842679601"></p><p>2） jps -q 仅显示进程id</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546842700335.png" alt="1546842700335"></p><p>3） jps -l 输出完全的包名，主类名，jar完全路径名</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546842734126.png" alt="1546842734126"></p><p>4） jps -v 显示jvm参数</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546842772451.png" alt="1546842772451"></p><p>jps用法简单，使用频率非常高。</p><h3 id="二、jstat"><a href="#二、jstat" class="headerlink" title="二、jstat"></a>二、jstat</h3><p>这个命令用于分析某个jvm进程的jvm的使用情况的统计信息，它可以显示本地或者远程[1]虚拟机进程中的类装载、 内存、 垃圾收集、 JIT编译等运行数据，在没有GUI图形界面，只提供了纯文本控制台环境的服务器上，它将是运行期定位虚拟机性能问题的首选工具。 命令格式为：</p><p><code>jstat [ generalOption | outputOptions vmid [ interval[s|ms] [ count ] ]</code></p><p>具体介绍：</p><p>generalOption：其实就是-h或者-options</p><p>outputOptions：由单个statOption或加上其他-t、-h和-J组成的options，用于确定输出的内容和格式</p><p>输出是表格的形式，表头字段用于描述列内容。-h设置表头的打印频率，例如-h3用于表示每3行打印一次表头部。使用-t选项显示时间戳列，标记为Timestamp作为输出的第一列。Timestamp列包含自目标JVM启动以来经过的时间（以秒为单位）。时间戳的分辨率取决于各种因素，并且由于在高负载系统上延迟的线程调度而受到变化。</p><p>vmid: Virtual machine identifier的缩写，一般就是本地进程ID或者是远程服务器进程</p><p>interval[s|ms]： 秒或者毫秒，即多少秒/毫秒打印一次，默认为毫秒</p><p>count: 一共打印多少次</p><p>官方不建议编写脚本来解析jstat命令的输出，因为格式会在不同版本中更改。</p><p>一些-<em>statOption</em>：</p><table><thead><tr><th>参数名称</th><th>参数作用</th></tr></thead><tbody><tr><td>class</td><td>显示有关类加载方面的统计信息</td></tr><tr><td>compiler</td><td>显示有关Java HotSpot VM实时编译器行为的统计信息</td></tr><tr><td>gc</td><td>显示有关垃圾回收堆行为的统计信息</td></tr><tr><td>gccapacity</td><td>显示有关代的容量及其相应空间的统计信息</td></tr><tr><td>gccause</td><td>显示有关垃圾收集统计信息（与-gcutil相同）的摘要，其中包含最后一个和当前垃圾收集事件的原因</td></tr><tr><td>gcnew</td><td>显示年轻代行为的统计信息</td></tr><tr><td>gcnewcapacity</td><td>显示有关年轻代及其相应空间大小的统计信息</td></tr><tr><td>gcold</td><td>显示有关老年代和元数据空间统计信息行为的统计信息</td></tr><tr><td>gcoldcapacity</td><td>显示有关老年代大小的统计信息</td></tr><tr><td>gcmetacapacity</td><td>显示有关元数据空间大小的统计信息</td></tr><tr><td>gcutil</td><td>显示有关垃圾收集统计信息的摘要</td></tr><tr><td>printcompilation</td><td>显示Java HotSpot VM编译方法统计信息</td></tr></tbody></table><p>举例如下：</p><p>如下表示分析进程id为31736 的gc情况，每隔1000ms打印一次记录，打印10次停止，每3行后打印指标头部</p><pre class=" language-bash"><code class="language-bash">jstat -gc -h3 11919 1000 10</code></pre><p>结果如下所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546849787600.png" alt="1546849787600"></p><p>结果指标含义如下：</p><table><thead><tr><th align="center">参数</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">S0C</td><td align="center">年轻代中第一个survivor（幸存区）的容量 (字节)</td></tr><tr><td align="center">S1C</td><td align="center">年轻代中第二个survivor（幸存区）的容量 (字节)</td></tr><tr><td align="center">S0U</td><td align="center">年轻代中第一个survivor（幸存区）目前已使用空间 (字节)</td></tr><tr><td align="center">S1U</td><td align="center">年轻代中第二个survivor（幸存区）目前已使用空间 (字节)</td></tr><tr><td align="center">EC</td><td align="center">年轻代中Eden（伊甸园）的容量 (字节)</td></tr><tr><td align="center">EU</td><td align="center">年轻代中Eden（伊甸园）目前已使用空间 (字节)</td></tr><tr><td align="center">OC</td><td align="center">老年代的容量 (字节)</td></tr><tr><td align="center">OU</td><td align="center">老年代目前已使用空间 (字节)</td></tr><tr><td align="center">PC</td><td align="center">Perm(持久代)的容量 (字节)</td></tr><tr><td align="center">PU</td><td align="center">Perm(持久代)目前已使用空间 (字节)</td></tr><tr><td align="center">YGC</td><td align="center">从应用程序启动到采样时年轻代中gc次数</td></tr><tr><td align="center">YGCT</td><td align="center">从应用程序启动到采样时年轻代中gc所用时间(s)</td></tr><tr><td align="center">FGC</td><td align="center">从应用程序启动到采样时老年代(全gc)gc次数</td></tr><tr><td align="center">FGCT</td><td align="center">从应用程序启动到采样时老年代(全gc)gc所用时间(s)</td></tr><tr><td align="center">GCT</td><td align="center">从应用程序启动到采样时gc用的总时间(s）</td></tr></tbody></table><p>其他命令统计结果可以<a href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html#BEHHGFAE" target="_blank" rel="noopener">查阅文档</a>， 一般都是生产环境下不方面使用其他工具的时候使用此命令大致查看gc信息。</p><h3 id="三、jstatd"><a href="#三、jstatd" class="headerlink" title="三、jstatd"></a>三、jstatd</h3><p>这是一个远程调用服务程序，监视Java虚拟机（JVM）并使远程监视工具能够连接到JVM。比如jps、jinfo等对远程服务器使用，那么远程服务器上就要启动jstatd才可以。</p><p>用法：</p><p><code>jstatd [options]</code></p><p>参数命令：</p><p>-nr : 当找不到现有的RMI注册表时，不会尝试在jstatd进程中创建内部RMI注册表。</p><p>-p port ： 如果未指定-nr选项，则在创建RMI注册表或未找到RMI注册表时创建的端口号。</p><p>-n rminame： 远程RMI对象在RMI注册表中绑定到的名称，默认名称为JStatRemoteHost。如果一个服务器上启动了多个jstatd服务，可以使用此参数为不同的jstatd服务指定唯一标识。</p><p>-Joption: 传递JVM的参数，其中option是Java应用程序启动器参数。例如，-J-Xms48m将启动内存设置为48 MB</p><h2 id="故障排除命令"><a href="#故障排除命令" class="headerlink" title="故障排除命令"></a>故障排除命令</h2><h3 id="一、jinfo"><a href="#一、jinfo" class="headerlink" title="一、jinfo"></a>一、jinfo</h3><p>jinfo 是jdk自带的一个工具，它可以用来查看<code>正在运行的java应用程序的扩展参数</code>（JVM中-X、-XX标示的参数），甚至支持在运行时修改部分参数。配置信息包括Java系统属性和Java虚拟机（JVM）命令行参数。对于64位JVM，可以加一个可选参数 <code>-J-d64</code>，例如<code>jinfo</code> <code>-J-d64 -sysprops pid</code>.</p><p><a href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jinfo.html#BCGEBFDD" target="_blank" rel="noopener">官方文档地址</a></p><p>用法如下：</p><p><code>jinfo [option] &lt;pid&gt;</code> 用于连接正在运行的进程</p><p><code>jinfo [option] &lt;executable &lt;core&gt;</code> 用于连接一个核文件</p><p><code>jinfo [option][server_id@]&lt;remote server IP or hostname&gt;</code> 用于连接一个远程debug服务器</p><p>option参数如下：</p><p>-flag <name>          打印指定的name参数的名称和值<br>-flag [+|-]<name>    使用或取消名称为name的Boolean参数<br>-flag <name>=<value> 设置name参数为给定value<br>-flags                       打印所有传递给JVM的命令行参数<br>-sysprops                打印Java系统属性<br><no option="">        打印上面所有内容<br>-h | -help                 打印帮助信息</no></value></name></name></name></p><p>1） jinfo -flags pid</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546844867487.png" alt="1546844867487"></p><p>2） jinfo -flag -XX:CICompilerCount pid  查看某个具体的参数的值</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546845113000.png" alt="1546845113000"></p><h3 id="二、jmap"><a href="#二、jmap" class="headerlink" title="二、jmap"></a>二、jmap</h3><p>jmap（Memory Map for Java）命令用于生成堆转储快照（一般称为heapdump或dump文件） ，此文件是分析应用程序可能发生的问题以及调优的重要文件。jmap的作用并不仅仅是为了获取dump文件，它还可以查询finalize执行队列、 Java堆和永久代的详细信息，如空间使用率、 当前用的是哪种收集器等 。</p><p>命令格式：</p><p><strong>jmap</strong> [ <em>options</em> ] <em>pid</em></p><p>option可选值介绍如下：</p><table><thead><tr><th align="center">参数</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">-dump</td><td align="center">生成java堆转储快照。格式为：-dump:[live,] format=b, file=<em>filename</em>，live参数作用是控制是否只dump出存活的对象，存储文件为hporf二进制文件。可以配合jhap命令浏览分析转储出的文件。</td></tr><tr><td align="center">-finalizerinfo</td><td align="center">显示在F-Queue中等待Finalizer线程执行finalize方法的对象。只在Linux/Solaris平台下有效。</td></tr><tr><td align="center">-heap</td><td align="center">显示Java堆中的详细信息，包括使用哪种收集器、参数配置、分代状况等，只在Linux/Solaris平台下有效。</td></tr><tr><td align="center">-histo</td><td align="center">显示堆中对象统计信息，包括类、实例数量、合计容量</td></tr><tr><td align="center">-permstat</td><td align="center">以ClassLoader为统计口径显示永久代内存状态。只在Linux/Solaris平台下有效。</td></tr><tr><td align="center">-F</td><td align="center">当虚拟机进程堆-dum参数没有响应时，可以使用这个选项强制生成dump快照。只在Linux/Solaris平台下有效。</td></tr><tr><td align="center">-clstats</td><td align="center">显示Java堆的类加载器的统计信息。对于每个类加载器，它的名称，活动程度，地址，父类加载器以及它加载的类的数量和大小都会被打印出来。</td></tr></tbody></table><h3 id="三、jhat"><a href="#三、jhat" class="headerlink" title="三、jhat"></a>三、jhat</h3><p>jhat（Java Heap Analysis Tool）一般与jmap搭配使用，用途分析jmap生成的堆转储快照。jhap内置一个http服务器，可以在浏览器中查看分析结果。但是这个工具分析结果比较简陋，而且一般不会直接在服务器上直接分析dump文件，所以比较鸡肋。有很多其他优秀的分析工具可以使用，比如VisualVM、MAT等，后面会介绍。</p><p>命令格式：</p><p>jhat filename</p><h2 id="可视化监控与分析Java应用程序"><a href="#可视化监控与分析Java应用程序" class="headerlink" title="可视化监控与分析Java应用程序"></a>可视化监控与分析Java应用程序</h2><h3 id="一、JConsole"><a href="#一、JConsole" class="headerlink" title="一、JConsole"></a>一、JConsole</h3><p>jconsole命令启动图形控制台工具，可以监视和管理本地或远程计算机上的Java应用程序和虚拟机。它基于JMX技术，它管理部分的功能是针对JMX MBean进行管理，MBean可以使用代码、 中间件服务器的管理控制台或者所有符合JMX规范的软件进行访问。</p><p>在Windows下，启动在jdk/bin 下的jconsole.exe以后将自动搜索出本机运行的所有虚拟机进程，不需要通过jps来查询，然后选择一个进程即可开始监控。如下图所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1547118823186.png" alt="1547118823186"></p><p>jconsole包含了非常丰富的信息，主界面如下图所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1547119121812.png" alt="1547119121812"></p><p>可以看到，可以查看概述、内存、线程、类、vm概要、MBean等六大类信息，每一类下面都包括丰富的图标和文字统计信息，非常强大。 </p><p>“概述”页签显示的是整个虚拟机主要运行数据的概览，其中包括“堆内存使用情况”、“线程”、 “类”、 “CPU使用情况”4种信息的曲线图。</p><p>“内存”页签相当<code>于可视化的jstat命令</code>，用于监视受收集器管理的虚拟机内存（Java堆和永久代）的变化趋势。  </p><p>“线程”页签的功能相当于可视化的jstack命令，遇到线程停顿时可以使用这个页签进行监控分析。</p><p> “VM概要”里面则显示了虚拟机的信息，如下图所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1547120203637.png" alt="1547120203637"></p><h3 id="二、-VisualVM"><a href="#二、-VisualVM" class="headerlink" title="二、 VisualVM"></a>二、 VisualVM</h3><p>VisualVM（All-in-One Java Troubleshooting Tool） 是到目前为止随JDK发布的功能最强大的<code>运行监视</code>和<code>故障处理</code>程序 ，它不仅可以用来运行监视、故障处理，还可以用来进行<code>性能分析</code>。</p><p>它还支持<code>插件拓展</code>。结合插件，VisualVM可以做到很多强大的功能，比如：</p><ul><li>显示虚拟机进程以及进程的配置、 环境信息（jps、 jinfo） </li><li>监视应用程序的CPU、 GC、 堆、 方法区以及线程的信息（jstat、 jstack） </li><li>dump以及分析堆转储快照（jmap、 jhat） </li><li>方法级的程序运行性能分析，找出被调用最多、 运行时间最长的方法 </li><li>离线程序快照：收集程序的运行时配置、 线程dump、 内存dump等信息建立一个快照，可以将快照发送开发者处进行Bug反馈 </li><li>……</li></ul><p>其程序主界面如下图所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1547127396252.png" alt="1547127396252"></p><p>选择本地某个进程点进去，发现其主要功能和JConsole差不多，但是其支持插件，所以有更多强大的功能，而且可视化界面更加友好。在监视选项卡下支持堆dump，点击之后会生成堆dump文件，并自动加载分析，得到分析结果，如下所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1547127653210.png" alt="1547127653210"></p><p>当VisualVM关闭时，此dump文件会自动删除，所以如果想要保存，需要在选项卡上右击另存为。</p><p>VisualVM还支持分析程序性能的功能。在Profiler页签中，VisualVM提供了程序运行期间方法级的CPU执行时间分析以及内存分析，做Profiling分析肯定会对程序运行性能有比较大的影响，所以一般不在生产环境中使用这项功能。 选择“CPU”和“内存”按钮中的一个，然后切换到应用程序中对程序进行操作，VisualVM会记录到这段时间中应用程序执行过的方法。 比如如果是CPU分析，将会统计每个方法的执行次数、 执行耗时；如果是内存分析，则会统计每个方法关联的对象数以及这些对象所占的空间。  </p><p>VisualVM有两个很重要的插件，<code>VisualGC</code>和<code>BTrace动态日志跟踪</code>。</p><p>VisualGC对进程的GC情况作了统计和可视化，效果如下所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1547127922111.png" alt="1547127922111"></p><p>BTrace的作用是在不停止目标程序运行的前提下，通过HotSpot虚拟机的HotSwap技术[4]动态加入原本并不存在的调试代码。  这项功能对实际生产中的程序很有意义：经常遇到程序出现问题，但排查错误的一些必要信息，譬如方法参数、 返回值等，在开发时并没有打印到日志之中，以至于不得不停掉服务，通过调试增量来加入日志代码以解决问题。 当遇到生产环境服务无法随便停止时，缺一两句日志导致排错进行不下去是一件非常郁闷的事情。</p><p>BTrace脚本编写和Java很像，但也有很多东西需要学，这里暂时不多做介绍。 </p><p>至于插件的安装，非常简单，在菜单栏的工具-&gt;插件点击即可，如下图所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1547128104143.png" alt="1547128104143"></p><p>选择想要安装的插件点击安装即可自动下载安装，也可以去VisualVM官网去下载插件然后放到指定的位置，这里不多做介绍，有兴趣的可以自己查阅资料。[插件官方地址](<a href="http://Visualvm" target="_blank" rel="noopener">http://Visualvm</a> java.net/pluginscenters.html )</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> JVM </tag>
            
            <tag> JDK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM参数类型分类</title>
      <link href="/2019/11/14/java-jvm-3/"/>
      <url>/2019/11/14/java-jvm-3/</url>
      
        <content type="html"><![CDATA[<h1 id="深入理解Java虚拟机读书笔记-3-JVM参数类型分类"><a href="#深入理解Java虚拟机读书笔记-3-JVM参数类型分类" class="headerlink" title="深入理解Java虚拟机读书笔记(3): JVM参数类型分类"></a>深入理解Java虚拟机读书笔记(3): JVM参数类型分类</h1><p>JVM有很多参数，一般可以分为三大类：标准参数、X参数和XX参数</p><h2 id="标准参数"><a href="#标准参数" class="headerlink" title="标准参数"></a>标准参数</h2><p>所谓标准参数，即一般化参数，往往是固定不变的，比如以下参数：</p><ul><li><p>-help</p></li><li><p>-version 显式虚拟机类型 当前版本号等等<br>这里注意JVM默认开启了mixmode混合模式，这意味着JVM在运行时可以动态的把字节码编译成本地代码</p></li><li><p>-server、-client</p><p>-server  默认为堆提供了一个更大的空间和并行的垃圾收集器，并且在运行时可以更大程度的优化代码</p><p>-client   客户端虚拟机有较小的默认堆内存，可以缩短JVM启动的时间和占用更少的内存，客户端的JVM只有在32位操作系统中才有</p><blockquote><p>注意</p></blockquote><p>1 ) 从JDK5开始 当应用启动时会检测当前的运行环境是否是服务器 如果是服务器就使用Server JVM 这是为了提升性能，一般来说Server JVM启动比Client JVM慢，原因是使用的是重量级的虚拟机，但是内部进行了很多优化，而Client JVM使用的是轻量级的JVM，当服务稳定运行后还是Server JVM的速度更快一些<br>2 ) 在JDK6中 Server JVM要求至少双核CPU和2GB物理内存</p><p>3 ) 在32位操作系统上 JDK可以运行Server JVM 但是JRE只能运行Client JVM</p></li><li><p>-cp、-classpath</p></li></ul><h2 id="X参数"><a href="#X参数" class="headerlink" title="X参数"></a>X参数</h2><p>X参数是非标准化参数，在java的各个jdk版本中可能会发生微小的变化，比如如下几个常用的：</p><ul><li>-Xlint：解释执行, int是interpretation的简称，翻译解释的意思，意味着强制JVM执行所有的字节码，这会降低运行速度[10倍左右]</li><li>-Xcomp：comp是Compile的简称，编译的意思第一次使用就编译成本地代码，从而带来最大程度的优化，虽然比-Xint的效率要高，但是它<code>没有让JVM启动JIT编译器的全部功能</code>， JIT编译器一般会在运行时创建方法使用文件，然后一步步的优化每个方法，因此该指令还是会造成一定的效率衰减</li><li>-Xmixed：混合模式，JVM自己来决定是否编译成本地代码，默认开启了混合模式，因此无需显式的指定</li></ul><p>Java是解释执行的，但是虚拟机里JIT即时编译的部分，可以把java代码转换成本地代码，上面的参数就是控制编译本地代码的参数。</p><h2 id="XX参数"><a href="#XX参数" class="headerlink" title="XX参数"></a>XX参数</h2><p>XX参数也是一种非标准化的参数，用户可以自己设置，JVM调优和debug都是用这些参数，主要分为以下两大类：</p><ul><li>Boolean类型</li></ul><p>格式：-XX:[+-]<name>方括号里面的+-表示启用或者禁用name属性</name></p><p>比如：-XX:+UseConcMarkSweepGC 表示穷CMS垃圾收集器 </p><p>​    -XX:+UseG1GC等</p><ul><li>非Boolean类型</li></ul><p>此类型一般是key-value类型</p><p>格式：-XX:<name>=<value>表示name属性的值是value</value></name></p><p>比如：XX:MaxGCPauseMillis=500 设置GC收集暂停时间为500</p><h2 id="特例"><a href="#特例" class="headerlink" title="特例"></a>特例</h2><p>还有一些非常常用的参数，比如-Xmx -Xms等，看个以为是X参数，其实是XX参数</p><p>-Xms 等价于 -XX:InitialHeapSize  初始化的堆大小</p><p>-Xmx 等价于 -XX:MaxHeapSize    最大化的堆大小</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>JVM参数非常多，但一般使用的是XX参数居多，用于调优和debug错误。具体使用哪些参数，需要具体问题具体分析，目前jdk自带的虚拟机实现都是HotSpot，可以查阅相关文档。</p><p>也可以下载某个知名论坛整理的文档，<a href="http://disq.us/url?url=http%3A%2F%2Ffiles.zeroturnaround.com%2Fpdf%2Fzt_JVM-options-cheat-sheet.pdf%3AsfOsGCixViJ6_yNr5059YY8qq7g&amp;cuid=344030" target="_blank" rel="noopener">下载地址</a></p><p><img src="https://zeroturnaround.com/wp-content/uploads/2016/12/JVM-Options-cheat-sheet-v2.png" alt="img"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解HotSpot虚拟机对象</title>
      <link href="/2019/11/14/java-jvm-2/"/>
      <url>/2019/11/14/java-jvm-2/</url>
      
        <content type="html"><![CDATA[<h1 id="深入理解Java虚拟机读书笔记-2-深入理解HotSpot虚拟机对象"><a href="#深入理解Java虚拟机读书笔记-2-深入理解HotSpot虚拟机对象" class="headerlink" title="深入理解Java虚拟机读书笔记(2): 深入理解HotSpot虚拟机对象"></a>深入理解Java虚拟机读书笔记(2): 深入理解HotSpot虚拟机对象</h1><p>为了理解虚拟机中数据的细节，比如如何创建、如何布局以及如何访问，必须具体到某一虚拟机和某一个内存区域。此处深入探讨HotSpot虚拟机在Java堆中对象分配、布局和访问的全过程。</p><h2 id="一、对象的创建"><a href="#一、对象的创建" class="headerlink" title="一、对象的创建"></a>一、对象的创建</h2><p>反映到Java语言中，对象的创建通常不过是一个<code>new</code>关键字，然而反映到底层虚拟机上是如何呢？可以概括为以下三步：</p><ul><li><p><strong>类加载：</strong> 虚拟机遇到一个new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个<code>类的符号引用</code>，并且检查这个符号引用代表的类<code>是否已被加载、 解析和初始化过</code>。 如果没有，那必须先执行相应的类加载过程。</p></li><li><p><strong>分配内存：</strong> 类加载通过后，虚拟机为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。 这里一般有两种划分方式：</p><ul><li>Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是<code>把那个指针向空闲空间那边挪动一段与对象大小相等的距离</code>，这种分配方式称为<code>“指针碰撞”(Bump the Pointer)</code>。</li><li>Java堆中内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，<code>记录哪些内存块是可用的</code>，在分配的时候<code>从列表中找到一块足够大的空间划分给对象实例</code>，并更新列表上的记录，这种分配方式称为<code>“空闲列表”（Free List）</code>。 </li></ul><p>选择哪种分配方式<code>由Java堆是否规整决定</code>，而Java堆是否规整又由所采用的垃圾收集器<code>是否带有压缩整理功能决定</code>。 因此，在使用Serial、 ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。 </p><p>在分配内存时，另外一个需要考虑的问题是线程安全性。对象创建时一个非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的。例如可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。  针对此问题有两种解决方案：</p><ul><li>对分配内存空间的动作进行同步处理：即虚拟机采用CAS+失败重试的方式保证更新操作的原子性</li><li>把内存分配的动作按照线程划分在不同得空间中进行，即每个线程在Java堆中预先分配一小块内存，称为<code>本地线程分配缓冲（Thread Local Allocation Buffer,TLAB）</code> 。这有点类似于线程封闭技术，哪个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。虚拟机是否使用TLAB，可以通过-XX：+/-UseTLAB参数来设定。 </li></ul></li><li><p><strong>初始化：</strong> 内存分配完成后，需要对分配到的内存空间都进行初始化为<code>零值</code>（不包括对象头），如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行。 这一步的动作，保证了对象的实例字段在Java代码中可以<code>不赋初值就可以直接使用</code>，程序能访问到这些字段的数据类型所对应的零值。 </p></li><li><p><strong>对象头设置</strong>：上面的初始化仅仅是通用的设置并且不包括对象头的设置，虚拟机接下来还要对对象进行更加丰富的设置，例如这个对象是哪个类的实例、 如何才能找到类的元数据信息、 对象的哈希码、 对象的GC分代年龄等信息。 这些信息存放在对象的<code>对象头（Object Header）</code>之中。 根据虚拟机当前的运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 </p></li><li><p><strong>对象init：</strong> 完成上面的工作以后，在虚拟机中其实一个对象已经产生了，但是从Java代码来看，其实<init>方法还没有执行，此时所有的字段还停留在初始化时设置的零值。一般来说（由字节码中是否跟随invokespecial指令所决定），执行new指令之后会接着执行＜init＞方法，<code>把对象按照程序员的意愿进行初始化</code>，这样一个真正可用的对象才算完全产生出来。 </init></p></li></ul><p>至此，一个对象创建完成。那么，对象在内存中又是如何布局的呢？</p><h2 id="二、对象的内存布局"><a href="#二、对象的内存布局" class="headerlink" title="二、对象的内存布局"></a>二、对象的内存布局</h2><p>在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：<code>对象头（Header）、实例数据（Instance Data）</code>和<code>对齐填充（Padding）</code>。 </p><h3 id="2-1-对象头"><a href="#2-1-对象头" class="headerlink" title="2.1 对象头"></a>2.1 对象头</h3><p>对象头包括两部分信息，即对象运行时信息和类型指针。</p><p>第一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、 锁状态标志、 线程持有的锁、 偏向线程ID、 偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为<code>“Mark Word”</code>。  对象头信息是与对象自身定义的数据无关的额外存储成本，考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。  </p><p>对象头的另外一部分是类型指针，即<code>对象指向它的类元数据的指针</code>，虚拟机<code>通过这个指针来确定这个对象是哪个类的实例</code>。  并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，<code>查找对象的元数据信息并不一定要经过对象本身。</code>另外，如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是<code>从数组的元数据中却无法确定数组的大小</code>。 </p><h3 id="2-2-实例数据"><a href="#2-2-实例数据" class="headerlink" title="2.2 实例数据"></a>2.2 实例数据</h3><p>实例数据部分是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容，包括子类和父类的。关于存储顺序，受到虚拟机分配策略参数和字段在源码中定义顺序的影响。一般来说，相同宽度的字段总是被分配到一起，在此前提下，父类中定义的变量会出现在子类之前。如果CompactFields参数值为true（默认为true），那么子类之中较窄的变量也可能会插入到父类变量的空隙之中。 </p><h3 id="2-3-对齐填充"><a href="#2-3-对齐填充" class="headerlink" title="2.3 对齐填充"></a>2.3 对齐填充</h3><p>这一部分不是必然存在的，仅仅起着占位符的作用，没有什么实际含义。由于HotSpot VM的自动内存管理系统要求<code>对象起始地址必须是8字节的整数倍</code>，换句话说，就是<code>对象的大小必须是8字节的整数倍</code>。 而对象头部分正好是8字节的倍数（1倍或者2倍），因此，<code>当对象实例数据部分没有对齐时，就需要通过对齐填充来补全</code>。 </p><h2 id="三、对象的访问"><a href="#三、对象的访问" class="headerlink" title="三、对象的访问"></a>三、对象的访问</h2><p>Java程序通过<code>栈上的reference</code>数据来操作<code>堆上的具体对象</code>。由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、 访问堆中的对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。 目前主流的访问方式有使用<code>句柄</code>和<code>直接指针</code>两种。 关于句柄，可以参考知乎的这个话题<a href="https://www.zhihu.com/question/27656256/answer/37556901" target="_blank" rel="noopener">句柄是什么？</a></p><h3 id="句柄访问"><a href="#句柄访问" class="headerlink" title="句柄访问"></a>句柄访问</h3><p>如果使用句柄访问，Java堆中会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。如下图所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546826004438.png" alt="1546826004438"></p><h3 id="直接指针访问"><a href="#直接指针访问" class="headerlink" title="直接指针访问"></a>直接指针访问</h3><p>如果使用直接指针访问，那么Java堆对象的布局中就必须考虑如何放置访问<code>类型数据</code>的相关信息，而reference中存储的直接就是对象地址，如下图所示：</p><p><img src="C:%5CUsers%5Ctenyun%5CDesktop%5C%E6%89%BE%E5%B7%A5%E4%BD%9C%5C%E7%AC%94%E8%AE%B0%5Cimgs%5C1546826074782.png" alt="1546826074782"></p><p>这两种对象访问方式各有优势，使用句柄来访问的最大好处就是reference中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要修改。 使用直接指针访问方式的最大好处就是速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本。 </p><p>对于Hotspot而言，使用的是直接指针访问。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> JVM </tag>
            
            <tag> HotSpot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存管理重要概念</title>
      <link href="/2019/11/14/java-jvm-1/"/>
      <url>/2019/11/14/java-jvm-1/</url>
      
        <content type="html"><![CDATA[<h1 id="深入理解Java虚拟机读书笔记-1-：内存管理重要概念"><a href="#深入理解Java虚拟机读书笔记-1-：内存管理重要概念" class="headerlink" title="深入理解Java虚拟机读书笔记(1)：内存管理重要概念"></a>深入理解Java虚拟机读书笔记(1)：内存管理重要概念</h1><p>说到Java内存管理，不得不先贴一张非常经典的图，如下所示：</p><p><img src="http://www.rowkey.me/images/blog_images/javamm/java-runtime-memory.jpg" alt="java-runtime-memory.jpg"></p><p>这些模块有些是线程私有的，有的则是线程共享的。下面一一对这些模块进行介绍：</p><h2 id="一、程序计数器"><a href="#一、程序计数器" class="headerlink" title="一、程序计数器"></a>一、程序计数器</h2><p>程序计数器一块比较小的内存空间，可以看做当前线程所执行的字节码的行号指示器，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、 循环、 跳转、 异常处理、 线程恢复等基础功能都需要依赖这个计数器来完成。 </p><p>多线程执行时，实际上是轮流占用处理器来执行的，因此，不可避免的有线程切换，因为任何一个时刻，有且仅有一个线程中的指令占用处理器。为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立<br>的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 </p><p>当执行的是java代码时，计数器记录的就是正在执行的虚拟机字节码指令的地址；如果是native方法，计数器值为空。</p><p>此处内存区域是<strong>唯一一个</strong>在Java虚拟机规范中没有规定任何内存溢出(OutOfMemoryError)情况的区域。</p><h2 id="二、Java虚拟机栈"><a href="#二、Java虚拟机栈" class="headerlink" title="二、Java虚拟机栈"></a>二、Java虚拟机栈</h2><p>虚拟机栈是线程私有的，它的生命周期和线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个<code>栈帧（Stack Frame）</code>用于存储局部变量表、 操作数栈、 动态链接、 方法出口等信息。 每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 </p><p>常说的的Java内存区域分为堆内存（Heap）和栈内存(Stack)，这里的<strong>栈</strong>就是虚拟机栈，或者是虚拟机栈中局部变量表的部分。</p><p>所谓局部变量表，顾名思义，其中存放了编译期可以知道的各种基本数据类型（8种基本数据类型）、对象引用（reference类型，它不等同于对象本身，可能是一个<code>指向对象起始地址的引用指针</code>，也可能是<code>指向一个代表对象的句柄或其他与此对象相关的位置</code> ）和returnAddress类型(指向了一条字节码指令的地址 )。<code>局部变量表所需的内存空间在编译期间完成分配</code>，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在<code>方法运行期间不会改变局部变量表的大小</code>。 </p><p>Java虚拟机规范规定了两种此区域会发生的异常：</p><ul><li>StackOverflowError异常：线程请求的栈深度大于虚拟机所允许的深度 </li><li>OutOfMemoryError异常：如果虚拟机栈可以动态扩展（当前大部分虚拟机都可以动态扩展） ，如果扩展时无法申请到足够的内存 </li></ul><h2 id="三、本地方法栈"><a href="#三、本地方法栈" class="headerlink" title="三、本地方法栈"></a>三、本地方法栈</h2><p>本地方法栈（Native Method Stack）与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。 </p><p>这个可以说是Java为了妥协而产生的能够调用其他代码的产物，可以不需要深入理解。</p><h2 id="四、Java堆"><a href="#四、Java堆" class="headerlink" title="四、Java堆"></a>四、Java堆</h2><p>Java堆是虚拟机所管理的内存最大的一块，被所有线程所共享，虚拟机启动时自动创建。此内存区域主要目的是存放对象实例。根据Java虚拟机规范中描述，<code>所有的对象实例及数组都要在堆上分配</code>。但是随着技术的发展，JIT编译期日趋成熟，逃逸分析技术（后面介绍）逐渐成熟，栈上分配、标量替换优化技术等，所有对象分配在堆上也不是“绝对”了。</p><p>Java对既然存放了可以说是所有的对象，那么自然而然的，此处是垃圾收集器管理的主要区域。因此有时候Java堆也被称为“GC堆”。大部分的收集器都采用分代收集算法，所以Java堆还可以细分为新生代和老年代。其实还可以更加细分，其目的和作用不过是为了更加方便的跟快的分配和回收内存，与存放的内容无关，存放的都是对象实例。</p><p>Java堆可以是物理上不连续但逻辑上连续的内存空间。</p><h2 id="五、方法区"><a href="#五、方法区" class="headerlink" title="五、方法区"></a>五、方法区</h2><p>方法区也是各个线程共享的内存区域，用于存储已被虚拟机加载的<code>类信息、常量、静态变量、及时编译器编译后的代码</code>等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 </p><p>对于常见的HotSpot虚拟机来说，方法区又称为“永久代（Permanent Generation)”，其原因是HotSpot虚拟机的设计团队选择<code>把GC分代收集扩展至方法区</code>，或者说<code>使用永久代来实现方法区</code>而已，这样HotSpot的垃圾收集器可以像管理Java堆一样管理这部分内存，能够省去专门为方法区编写内存管理代码的工作。  对于其他虚拟机（如J9和JRockit）来说，不存在永久代的概念。</p><p>目前来看，使用永久代来实现方法区，不是一个好的主意，因为容易遇到内存溢出问题（永久代有-XX：MaxPermSize的上限，J9和JRockit只要没有触碰到进程可用内存的上限，例如32位系统中的4GB，就不会出现问题 ）。在JDK1.7中的HotSpot中，已经把原本放在永久代的字符串常量池移除。</p><p>方法区的管理非常宽松，垃圾收集行为在此区域比较少见。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载，但是类型卸载等条件非常苛刻，所以回收效果很差，但是又必须回收。根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 </p><h2 id="六、运行时常量池"><a href="#六、运行时常量池" class="headerlink" title="六、运行时常量池"></a>六、运行时常量池</h2><p>运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、 字段、 方法、 接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放<code>编译期生成的各种字面量和符号引用</code>，这部分内容将在类加载后进入方法区的运行时常量池中存放。 一般来说，除了保存Class文件中描述的<code>符号引用</code>外，还会把翻译出来的<code>直接引用</code>也存储在运行时常量池中。 </p><p>运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，<code>运行期间也可能将新的常量放入池中。</code></p><p>既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError异常。 </p><h2 id="七、直接内存"><a href="#七、直接内存" class="headerlink" title="七、直接内存"></a>七、直接内存</h2><p>直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。  </p><p>在JDK 1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的<code>DirectByteBuffer对象</code>作为这块内存的引用进行操作。 这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，肯定还是会受到本机总内存（包括RAM以及SWAP区或者分页文件）大小以及处理器寻址空间的限制。 </p><p> 服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但<code>经常忽略直接内存</code>，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统级的限制），从而导致动态扩展时出现OutOfMemoryError异常。 </p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> JVM </tag>
            
            <tag> 内存管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>papers-note-14</title>
      <link href="/2019/11/13/papers-note-14/"/>
      <url>/2019/11/13/papers-note-14/</url>
      
        <content type="html"><![CDATA[<h2 id="论文阅读笔记-14"><a href="#论文阅读笔记-14" class="headerlink" title="论文阅读笔记(14)"></a>论文阅读笔记(14)</h2><blockquote><p>论文题目：MARINE: Multi-relational Network Embeddings with Relational Proximity and Node Atributes</p><p>代码公开</p><p>发表于2019，www会议</p></blockquote><h3 id="主要创新点"><a href="#主要创新点" class="headerlink" title="主要创新点"></a>主要创新点</h3><p>对于齐次图和多关系图，提出新的表示学习模型。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> papers-notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/09/21/hello-world/"/>
      <url>/2019/09/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy<span class="token keyword">done</span></code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
